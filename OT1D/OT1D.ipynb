{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13309,
     "status": "ok",
     "timestamp": 1758296193137,
     "user": {
      "displayName": "Shu Liu",
      "userId": "06233144459630989636"
     },
     "user_tz": 240
    },
    "id": "aSYM5-A-QTlm",
    "outputId": "7562b1d4-330b-445a-cd7b-d5c2df1922c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 720,
     "status": "ok",
     "timestamp": 1758296193870,
     "user": {
      "displayName": "Shu Liu",
      "userId": "06233144459630989636"
     },
     "user_tz": 240
    },
    "id": "5FHjjp6gI1nO",
    "outputId": "2138c985-d910-460c-9087-072c560fdda2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/NPDG_codes_collection/OT1D\n"
     ]
    }
   ],
   "source": [
    "cd /content/drive/MyDrive/NPDG/OT1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "QSjLeuOZRg0t"
   },
   "outputs": [],
   "source": [
    "# @title import\n",
    "import math\n",
    "import random\n",
    "import scipy\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.func import grad, hessian, vmap\n",
    "from torch.func import jacrev\n",
    "from torch.func import functional_call\n",
    "import time\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pickle\n",
    "\n",
    "from numpy import *\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "OlXCS21tnzEz"
   },
   "outputs": [],
   "source": [
    "# @title set dimension of the problem\n",
    "\n",
    "\n",
    "dim = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1758296207067,
     "user": {
      "displayName": "Shu Liu",
      "userId": "06233144459630989636"
     },
     "user_tz": 240
    },
    "id": "QqmvN4WRcGEy",
    "outputId": "d9d3a05b-d9d8-4ba9-ae57-79d317f12e38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# @title Check CUDA availability\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "-HZo9kPmpqk_"
   },
   "outputs": [],
   "source": [
    "# @title model (net)\n",
    "\n",
    "\n",
    "class network_prim(nn.Module):\n",
    "    def __init__(self, network_length, input_dimension, hidden_dimension, output_dimension):\n",
    "        super(network_prim, self).__init__()\n",
    "\n",
    "        self.network_length = network_length\n",
    "        self.linears = nn.ModuleList([nn.Linear(input_dimension, hidden_dimension)])\n",
    "        self.linears.extend([nn.Linear(hidden_dimension, hidden_dimension) for _ in range(1, network_length-1)])\n",
    "        self.linears.extend([nn.Linear(hidden_dimension, output_dimension, bias=False)])\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def initialization(self):\n",
    "        for l in self.linears:\n",
    "            l.weight.data.normal_()\n",
    "            if l.bias is not None:\n",
    "                l.bias.data.normal_()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        l = self.linears[0]\n",
    "        x = l(x)\n",
    "        for l in self.linears[1: self.network_length-1]:\n",
    "            x = self.softplus(x)\n",
    "            x = l(x)\n",
    "        x = self.softplus(x)\n",
    "        l = self.linears[self.network_length-1]\n",
    "        x = l(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class network_dual(nn.Module):\n",
    "    def __init__(self, network_length, input_dimension, hidden_dimension, output_dimension):\n",
    "        super(network_dual, self).__init__()\n",
    "\n",
    "        self.network_length = network_length\n",
    "        self.linears = nn.ModuleList([nn.Linear(input_dimension, hidden_dimension)])\n",
    "        self.linears.extend([nn.Linear(hidden_dimension, hidden_dimension) for _ in range(1, network_length-1)])\n",
    "        self.linears.extend([nn.Linear(hidden_dimension, output_dimension, bias=False)])\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def initialization(self):\n",
    "        for l in self.linears:\n",
    "            l.weight.data.normal_()\n",
    "            if l.bias is not None:\n",
    "                l.bias.data.normal_()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        l = self.linears[0]\n",
    "        x = l(x)\n",
    "        for l in self.linears[1: self.network_length-1]:\n",
    "            x = self.softplus(x)\n",
    "            x = l(x)\n",
    "        x = self.softplus(x)\n",
    "        l = self.linears[self.network_length-1]\n",
    "        x = l(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "2wfJtCKy0WyY"
   },
   "outputs": [],
   "source": [
    "# @title 1D plot of OT map on [-L, L]\n",
    "\n",
    "def One_D_plot_T_with_real_solution_compare(net_primal, net_primal2, l, num_of_intervals, Iter, flag_plot_real, save_path, device, d=dim):\n",
    "    interval_width = l\n",
    "    delta_x = 2 * l / num_of_intervals\n",
    "    x = torch.arange(-l, l + delta_x, delta_x)\n",
    "    x = x.unsqueeze(-1)\n",
    "\n",
    "    T_x = net_primal(x.to(device))\n",
    "    T_x = T_x.detach().cpu()\n",
    "\n",
    "    T2_x = net_primal2(x.to(device))\n",
    "    T2_x = T2_x.detach().cpu()\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    plt.xlim([-l, l])\n",
    "    plt.ylim([-1.2 * l, 1.2 * l])\n",
    "    plt.xlabel('x', fontsize = 20)\n",
    "    plt.ylabel('y', fontsize = 20)\n",
    "\n",
    "    plt.plot(x, T_x, color='blue', label='NPDG')\n",
    "\n",
    "    plt.plot(x, T2_x, color='green', label='Primal-Dual using Adam')\n",
    "\n",
    "    if flag_plot_real == 1:\n",
    "        OT_map_x = torch.zeros(x.size()[0], 1)\n",
    "        for i in range(x.size()[0]):\n",
    "            OT_map_x[i] = OT_map(x[i])\n",
    "        OT_map_x = OT_map_x.squeeze().cpu()\n",
    "        plt.plot(x, OT_map_x, color='r', label='real map')\n",
    "    plt.legend(fontsize=30)\n",
    "    plt.title('Plot of computed maps  \\n', fontsize = 40)\n",
    "\n",
    "    filename = os.path.join(save_path, \"({}th Iteration) graph of T_theta comparison \".format(Iter)+'.pdf')\n",
    "    plt.savefig(filename)\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def One_D_plot_T_with_real_solution(net_primal, l, num_of_intervals, Iter, flag_plot_real, save_path, device, d=dim):\n",
    "    interval_width = l\n",
    "    delta_x = 2 * l / num_of_intervals\n",
    "    x = torch.arange(-l, l + delta_x, delta_x)\n",
    "    x = x.unsqueeze(-1)\n",
    "\n",
    "    T_x = net_primal(x.to(device))\n",
    "    T_x = T_x.detach().cpu()\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    plt.xlim([-l, l])\n",
    "    plt.ylim([-1.2 * l, 1.2 * l])\n",
    "    plt.plot(x, T_x, color='blue')\n",
    "\n",
    "    if flag_plot_real == 1:\n",
    "        OT_map_x = torch.zeros(x.size()[0], 1)\n",
    "        for i in range(x.size()[0]):\n",
    "            OT_map_x[i] = OT_map(x[i])\n",
    "        OT_map_x = OT_map_x.squeeze().cpu()\n",
    "        plt.plot(x, OT_map_x, color='r')\n",
    "\n",
    "    filename = os.path.join(save_path, \"({}th Iteration) graph of T_theta \".format(Iter)+'.pdf')\n",
    "    plt.savefig(filename)\n",
    "\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Ukl93MWUML1H"
   },
   "outputs": [],
   "source": [
    "# @title define rho0 (sampler) , rho1 (sampler) , real solution\n",
    "\n",
    "sigma0_a = 0.5\n",
    "sigma0_b = 0.25\n",
    "sigma0_c = 0.5\n",
    "mu0_a = -1\n",
    "mu0_b = 1\n",
    "mu0_c = 1\n",
    "lambda_a = 2/3\n",
    "lambda_c = 1/3\n",
    "def rho1(n, d=dim):\n",
    "    z_a = torch.randn(2*n, d)\n",
    "    x_a = sigma0_a * z_a + mu0_a\n",
    "    z_c = torch.randn(n, d)\n",
    "    x_c = sigma0_c * z_c + mu0_c\n",
    "    samples = torch.cat((x_a, x_c), 0)\n",
    "    return samples.cuda()\n",
    "\n",
    "\n",
    "def rho0(n, d=dim):\n",
    "    return torch.randn(3*n, d).cuda()\n",
    "\n",
    "\n",
    "# # Optimal Transport map\n",
    "# # T(\\cdot) = F_1^{-1}F_0(\\cdot)\n",
    "# # F_1^{-1}(.) = erf^{-1}(2 * . - 1)\n",
    "# # F_0(.) = \\sum_k=1^m λ_k/2 (1 + erf((. - μ_k)/(\\sqrt{2} σ_k)))\n",
    "# def optimal_transport_map(x, d=dim):\n",
    "# # def grad_u_real(x, d=dim):\n",
    "\n",
    "#     erfa = torch.special.erf((x - mu0_a)/(np.sqrt(2) * sigma0_a))\n",
    "#     # erfb = torch.special.erf((x - mu0_b)/(np.sqrt(2) * sigma0_b))\n",
    "#     erfc = torch.special.erf((x - mu0_c)/(np.sqrt(2) * sigma0_c))\n",
    "\n",
    "#     # F_0_x = lambda_a / 2 * (1 + erfa) + lambda_b / 2 * (1 + erfb) + lambda_c / 2 * (1 + erfc)\n",
    "#     F_0_x = lambda_a / 2 * (1 + erfa) + lambda_c / 2 * (1 + erfc)\n",
    "\n",
    "#     y = 2 * F_0_x - 1\n",
    "#     z = torch.special.erfinv(y)\n",
    "#     return z.cuda()\n",
    "\n",
    "\n",
    "# Optimal Transport map  rho0 [normal gaussian] to rho1 [multimodal gaussian]\n",
    "# Inverse of the above map\n",
    "def optimal_transport_map_inv(x, d=dim):\n",
    "\n",
    "    erfa = torch.special.erf((x - mu0_a)/(np.sqrt(2) * sigma0_a))\n",
    "    erfc = torch.special.erf((x - mu0_c)/(np.sqrt(2) * sigma0_c))\n",
    "    F_0_x = lambda_a / 2 * (1 + erfa) + lambda_c / 2 * (1 + erfc)\n",
    "    y = 2 * F_0_x - 1\n",
    "    z = np.sqrt(2) * torch.special.erfinv(y)\n",
    "\n",
    "    return z.cuda()\n",
    "\n",
    "\n",
    "def OT_map(x, d=dim):\n",
    "    yk = x\n",
    "    xk = optimal_transport_map_inv(yk).cpu()\n",
    "    y_left = x - 4\n",
    "    y_right = x + 4\n",
    "    for k in range(100):\n",
    "        if xk > x:\n",
    "            y_right = yk\n",
    "        else:\n",
    "            y_left = yk\n",
    "        yk = (y_left + y_right) / 2\n",
    "        xk = optimal_transport_map_inv(yk).cpu()\n",
    "        err = xk - x\n",
    "\n",
    "        if (abs(err) < 1e-5):\n",
    "            break\n",
    "\n",
    "    return yk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "7_3ejpl-wbFO"
   },
   "outputs": [],
   "source": [
    "# @title histogram\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "rng = np.random.default_rng(800)\n",
    "\n",
    "\n",
    "sigma0_a = 0.5\n",
    "sigma0_c = 0.5\n",
    "mu0_a = -1\n",
    "mu0_c = 1\n",
    "lambda_a = 2/3\n",
    "lambda_c = 1/3\n",
    "def rho0_density(x, d=dim):\n",
    "    rho0_x = pow(1/(np.sqrt(2*math.pi)), d) * torch.exp( - torch.sum((x)*(x), -1)/(2))\n",
    "    return rho0_x\n",
    "\n",
    "def rho1_density(x, d=dim):\n",
    "    rho1_x = lambda_a * pow(1/(np.sqrt(2*math.pi) * sigma0_a), d) * torch.exp( - torch.sum((x-mu0_a)*(x-mu0_a), -1)/(2 * sigma0_a * sigma0_a)) + lambda_c * pow(1/(np.sqrt(2*math.pi) * sigma0_c), d) * torch.exp( - torch.sum((x-mu0_c)*(x-mu0_c), -1)/(2 * sigma0_c * sigma0_c))\n",
    "    return rho1_x\n",
    "\n",
    "\n",
    "# pushfwd_rho0_samples_vs_rho1_samples\n",
    "def plt_histogram(L, N, n_bins):\n",
    "\n",
    "    delta_x = 2 * L / N\n",
    "    x_nodes = (torch.arange(0, N+1, 1) - N/2) * delta_x\n",
    "    x_nodes = x_nodes.unsqueeze(-1)\n",
    "\n",
    "    density_func_x = rho0_density(x_nodes)\n",
    "    density_func_x = density_func_x.squeeze()\n",
    "\n",
    "    # Generate two normal distributions\n",
    "    x = rho1(N)\n",
    "    OT_inv_x = optimal_transport_map_inv(x)\n",
    "    OT_inv_x = OT_inv_x.detach().cpu().squeeze()\n",
    "    rho1_x = rho1(N).cpu().squeeze()\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    plt.hist(rho1_x, bins=n_bins, density=True, color='green', alpha=0.7)\n",
    "    plt.hist(OT_inv_x, bins=n_bins, density=True, color='blue', alpha=1)\n",
    "    plt.plot(x_nodes, density_func_x, color='red', alpha=0.5)\n",
    "    plt.xlim((-L, L))\n",
    "    plt.ylim((0, 1.5))\n",
    "    plt.title(\"Histogram of OT inverse pushforward rho_1 (blue) and rho_1 (green).\")\n",
    "    save_path = os.getcwd()\n",
    "    filename = os.path.join(save_path, \"hist of rho1 and OT inv pushfwded rho0.pdf\".format(iter))\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# pushfwd_rho0_samples_vs_rho1_samples\n",
    "def plt_histogram2(L, N, n_bins):\n",
    "\n",
    "    delta_x = 2 * L / N\n",
    "    x_nodes = (torch.arange(0, N+1, 1) - N/2) * delta_x\n",
    "    x_nodes = x_nodes.unsqueeze(-1)\n",
    "\n",
    "    density_func_x = rho1_density(x_nodes)\n",
    "    density_func_x = density_func_x.squeeze()\n",
    "\n",
    "    # Generate two normal distributions\n",
    "    x = rho0(N).cpu()\n",
    "    y = torch.zeros(N)\n",
    "    for i in range(N):\n",
    "        y[i] = OT_map(x[i])\n",
    "\n",
    "    y = y.detach().cpu().squeeze()\n",
    "    z = rho1(N).cpu().squeeze()\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    plt.hist(z, bins=n_bins, density=True, color='green', alpha=0.7)\n",
    "    plt.hist(y, bins=n_bins, density=True, color='blue', alpha=1)\n",
    "    plt.plot(x_nodes, density_func_x, color='red', alpha=0.5)\n",
    "\n",
    "    plt.xlim((-L, L))\n",
    "    plt.ylim((0, 1.5))\n",
    "    plt.title(\"Histogram of OT pushforward rho_0 (blue) and rho_1 (green).\")\n",
    "    save_path = os.getcwd()\n",
    "    filename = os.path.join(save_path, \"hist of rho1 and OT pushfwded rho0.pdf\".format(iter))\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plt_histogram_pushfwd_rho0_rho1_densitycurve(iter, L, N, net_T, n_bins):\n",
    "\n",
    "    delta_x = 2 * L / N\n",
    "    x_nodes = (torch.arange(0, N+1, 1) - N/2) * delta_x\n",
    "    x_nodes = x_nodes.unsqueeze(-1)\n",
    "    density_func_x = rho1_density(x_nodes)\n",
    "    density_func_x = density_func_x.squeeze()\n",
    "\n",
    "    # Generate two normal distributions\n",
    "    x = rho0(N)\n",
    "    T_x = net_T(x)\n",
    "    T_x = T_x.detach().cpu().squeeze()\n",
    "    rho1_x = rho1(N).cpu().squeeze()\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    plt.hist(rho1_x, bins=n_bins, density=True, color='green', alpha=0.7)\n",
    "    plt.hist(T_x, bins=n_bins, density=True, color='blue', alpha=1)\n",
    "    plt.plot(x_nodes, density_func_x, color='red', alpha=0.5)\n",
    "    plt.xlim((-L, L))\n",
    "    plt.ylim((0, 1.5))\n",
    "    plt.title(\"Histogram of net_T pushforward rho_0 (blue) and rho_1 (green).\")\n",
    "    save_path = os.getcwd()\n",
    "    filename = os.path.join(save_path, \"(iteration = {}) hist of rho1 and numerical solution.pdf\".format(iter))\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "77uiwIHFhZzr"
   },
   "outputs": [],
   "source": [
    "# @title L2 error\n",
    "\n",
    "\n",
    "def gradient_nn(network, x):\n",
    "    input_variable = autograd.Variable(x, requires_grad=True)\n",
    "    output_value = network(input_variable)\n",
    "    gradients_x = autograd.grad(outputs=output_value, inputs=input_variable, grad_outputs=torch.ones(output_value.size()).cuda(), create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    return gradients_x\n",
    "\n",
    "\n",
    "def L2_error(net_u, N):  # L2 norm of \\nabla net_u - \\nabla u_real\n",
    "\n",
    "    samples = rho0(N)\n",
    "    grad_realsolution = grad_u_real(samples)\n",
    "    grad_u_x = gradient_nn(net_u, samples)\n",
    "    L2_error = torch.sqrt(((grad_u_x - grad_realsolution)*(grad_u_x - grad_realsolution)).mean())\n",
    "\n",
    "    return L2_error\n",
    "\n",
    "\n",
    "def L2_error_T(net_T, N):  # L2 norm of \\nabla net_u - \\nabla u_real\n",
    "\n",
    "    samples = rho0(N)\n",
    "    ot_map_x = optimal_transport_map(samples)\n",
    "    # print(ot_map_x.size())\n",
    "    T_x = net_T(samples)\n",
    "    # print(T_x.size())\n",
    "    L2_error = torch.sqrt(((T_x - ot_map_x)*(T_x - ot_map_x)).mean())\n",
    "\n",
    "    return L2_error\n",
    "\n",
    "\n",
    "def L2_error_T_with_OT_map_inv(net_T, N):  # L2 norm of \\nabla net_u - \\nabla u_real\n",
    "\n",
    "    y = rho1(N)\n",
    "    x = optimal_transport_map_inv(y)\n",
    "    T_x = net_T(x)\n",
    "    L2_error = torch.sqrt(((T_x - y)*(T_x - y)).mean())\n",
    "\n",
    "    return L2_error\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "OqjDUweNxBtm"
   },
   "outputs": [],
   "source": [
    "# @title PDHG loss\n",
    "\n",
    "\n",
    "def gradient_nn(network, x):\n",
    "\n",
    "    input_variable = autograd.Variable(x, requires_grad=True)\n",
    "    output_value = network(input_variable)\n",
    "    gradients_x = autograd.grad(outputs=output_value, inputs=input_variable, grad_outputs=torch.ones(output_value.size()).cuda(), create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    return gradients_x\n",
    "\n",
    "\n",
    "# PDHG loss with no boundary error\n",
    "def PDHG_loss1(net_u, net_phi, rho0_samples, rho1_samples):\n",
    "    grad_u_x = gradient_nn(net_u, rho0_samples)\n",
    "    loss = net_phi(grad_u_x) - net_phi(rho1_samples)\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "# derived from the Monge problem\n",
    "def PDHG_loss2(net_u, net_phi, rho0_samples, rho1_samples):\n",
    "    grad_u_x = gradient_nn(net_u, rho0_samples)\n",
    "    loss = - torch.sum(rho0_samples * grad_u_x, -1).mean() + net_phi(grad_u_x).mean() - net_phi(rho1_samples).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def PDHG_loss3(net_T, net_phi, rho0_samples, rho1_samples):\n",
    "    T_x = net_T(rho0_samples)\n",
    "    loss = - torch.sum(rho0_samples * T_x, -1).mean() + net_phi(T_x).mean() - net_phi(rho1_samples).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def PDHG_loss4(net_T, net_phi, rho0_samples, rho1_samples):\n",
    "    T_x = net_T(rho0_samples)\n",
    "    sqr_term = 1/2*torch.sum((T_x - rho0_samples)*(T_x - rho0_samples), -1).mean()\n",
    "    loss = sqr_term + net_phi(T_x).mean() - net_phi(rho1_samples).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def PDHG_loss4_with_extrapolation(net_T, net_phi, net_phi0, rho0_samples, rho1_samples, omega):\n",
    "    T_x = net_T(rho0_samples)\n",
    "    sqr_term = 1/2*torch.sum((T_x - rho0_samples)*(T_x - rho0_samples), -1).mean()\n",
    "    phi_T_x = net_phi(T_x)\n",
    "    phi0_T_x = net_phi0(T_x)\n",
    "    tilde_phi_T_x = phi_T_x +  omega * (phi_T_x - phi0_T_x)\n",
    "    phi_y = net_phi(rho1_samples)\n",
    "    phi0_y = net_phi0(rho1_samples)\n",
    "    tilde_phi_y = phi_y +  omega * (phi_y - phi0_y)\n",
    "    loss = sqr_term + tilde_phi_T_x.mean() - tilde_phi_y.mean()\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FVbj7yYdsqVz"
   },
   "outputs": [],
   "source": [
    "# @title G(\\theta) as a linear opt another\n",
    "import scipy\n",
    "from scipy.sparse.linalg import LinearOperator\n",
    "import torch.autograd.functional as functional\n",
    "\n",
    "\n",
    "def v_compute_Laplacian(net, samples):\n",
    "\n",
    "    def compute_Laplacian(x):\n",
    "        hessian_net = hessian(net, argnums=0)(x) #forward-over-reverse hessian calc.\n",
    "        laplacian_net = hessian_net.diagonal(0,-2,-1) #use relative dims for vmap (function doesn't see the batch dim of the input)\n",
    "        return torch.sum(laplacian_net, -1)\n",
    "\n",
    "    Laplacian_wrt_x = vmap(compute_Laplacian)(samples)\n",
    "\n",
    "    return Laplacian_wrt_x\n",
    "\n",
    "\n",
    "def tensor_to_numpy(u):\n",
    "    if u.device==\"cpu\":\n",
    "        return u.detach().numpy()\n",
    "    else:\n",
    "        return u.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "# explicitly form the Gram metric G(\\theta). Only for verification.\n",
    "def form_metric_tensor(input_dim, net, G_samples, device):\n",
    "\n",
    "    N = G_samples.size()[0]\n",
    "\n",
    "    Jacobi_NN = jacrev(functional_call, argnums = 1)\n",
    "    D_param_D_x_NN = vmap(jacrev(Jacobi_NN, argnums = 2), in_dims = (None, None, 0))\n",
    "    D_param_D_x_net_on_x = D_param_D_x_NN(net, dict(net.named_parameters()), G_samples)\n",
    "    num_params = torch.nn.utils.parameters_to_vector(net.parameters()).size()[0]\n",
    "    list_of_vectorized_param_gradients = []\n",
    "    for param_gradients in dict(D_param_D_x_net_on_x).items():\n",
    "        vectorized_param_gradients = param_gradients[1].view(N, -1, input_dim)\n",
    "        list_of_vectorized_param_gradients.append(vectorized_param_gradients)\n",
    "    total_vectorized_param_gradients = torch.cat(list_of_vectorized_param_gradients, 1)\n",
    "    transpose_total_vectorized_param_gradients = torch.transpose(total_vectorized_param_gradients, 1, 2)\n",
    "    batched_metric_tensor = torch.matmul(total_vectorized_param_gradients, transpose_total_vectorized_param_gradients)\n",
    "    metric_tensor = torch.mean(batched_metric_tensor, 0)\n",
    "\n",
    "    return metric_tensor\n",
    "\n",
    "\n",
    "# \\mathcal M = grad operator\n",
    "def metric_tensor_as_nabla_op(net, net_auxil, G_samples, vec, device):\n",
    "\n",
    "    num_params = len(torch.nn.utils.parameters_to_vector(net.parameters()))\n",
    "\n",
    "    params_net = dict(net.named_parameters())\n",
    "    params_net_auxil = dict(net_auxil.named_parameters())\n",
    "    ################### computation starts here ##################################\n",
    "    net.zero_grad()\n",
    "    net_auxil.zero_grad()\n",
    "\n",
    "    grad_net = vmap(jacrev(functional_call, argnums=2), in_dims=(None, None, 0))\n",
    "    grad_net_x = grad_net(net, params_net, G_samples)\n",
    "    grad_net_auxil_x =grad_net(net_auxil, params_net_auxil, G_samples)\n",
    "    ave_sqr_grad_net = torch.sum(grad_net_x * grad_net_auxil_x) / G_samples.size()[0]  # torch.sum(grad_net_x * grad_net_auxil_x, 2).mean()\n",
    "\n",
    "    nabla_theta_ave_sqr_grad_net = torch.autograd.grad(ave_sqr_grad_net, net_auxil.parameters(), grad_outputs=None ,allow_unused=True, retain_graph=True, create_graph=True)\n",
    "    vectorize_nabla_theta_ave_sqr_grad_net = torch.nn.utils.parameters_to_vector(nabla_theta_ave_sqr_grad_net)\n",
    "    vec_dot_nabla_theta_ave_sqr_grad_net = vectorize_nabla_theta_ave_sqr_grad_net.dot(vec)\n",
    "    metric_tensor_mult_vec = torch.autograd.grad(vec_dot_nabla_theta_ave_sqr_grad_net, net.parameters(), grad_outputs=None,allow_unused=True, retain_graph=True, create_graph=True)\n",
    "    vectorize_metric_tensor_mult_vec = torch.nn.utils.parameters_to_vector(metric_tensor_mult_vec)\n",
    "\n",
    "    return vectorize_metric_tensor_mult_vec\n",
    "\n",
    "\n",
    "# \\mathcal M = identity operator\n",
    "def metric_tensor_as_op_identity_part(net, net_auxil, G_samples, vec, device):\n",
    "\n",
    "    num_params = len(torch.nn.utils.parameters_to_vector(net.parameters()))\n",
    "\n",
    "    params_net = dict(net.named_parameters())\n",
    "    params_net_auxil = dict(net_auxil.named_parameters())\n",
    "    ################### computation starts here ##################################\n",
    "    net_x = net(G_samples)\n",
    "    net_auxil_x = net_auxil(G_samples)\n",
    "    ave_net = torch.sum(net_x * net_auxil_x) / G_samples.size()[0]\n",
    "    nabla_theta_ave_net = torch.autograd.grad(ave_net, net_auxil.parameters(), grad_outputs=None ,allow_unused=True, retain_graph=True, create_graph=True)\n",
    "    vectorize_nabla_theta_net = torch.nn.utils.parameters_to_vector(nabla_theta_ave_net)\n",
    "    vec_dot_nabla_theta_ave_net = vectorize_nabla_theta_net.dot(vec)\n",
    "    metric_tensor_mult_vec = torch.autograd.grad(vec_dot_nabla_theta_ave_net, net.parameters(), grad_outputs=None,allow_unused=True, retain_graph=True, create_graph=True)\n",
    "    vectorize_metric_tensor_mult_vec = torch.nn.utils.parameters_to_vector(metric_tensor_mult_vec)\n",
    "\n",
    "    return vectorize_metric_tensor_mult_vec\n",
    "\n",
    "\n",
    "# G1: \\mathcal M = Id operator\n",
    "# G2: \\mathcal M = ▽ operator\n",
    "def minres_solver_G(net, net_auxil, interior_samples, boundary_samples, RHS_vec, device, bd_lambda, max_iternum, minres_tolerance, G_type):\n",
    "\n",
    "    num_params = torch.nn.utils.parameters_to_vector(net.parameters()).size()[0]\n",
    "\n",
    "    def G1_as_operator(vec):  # input the vector v [on CPU], return vector Gv\n",
    "        tensorized_vec = torch.Tensor(vec).to(device)\n",
    "        Gv = metric_tensor_as_op_identity_part(net, net_auxil, interior_samples, tensorized_vec, device)\n",
    "        return tensor_to_numpy(Gv)\n",
    "\n",
    "    def G2_as_operator(vec):  # input the vector v [on CPU], return vector Gv\n",
    "        tensorized_vec = torch.Tensor(vec).to(device)\n",
    "        Gv = metric_tensor_as_nabla_op(net, net_auxil, interior_samples, tensorized_vec, device)\n",
    "        return tensor_to_numpy(Gv)\n",
    "\n",
    "    if G_type == \"1\":\n",
    "        G_operator = LinearOperator((num_params, num_params), matvec=G1_as_operator)\n",
    "    elif G_type == \"2\":\n",
    "        G_operator = LinearOperator((num_params, num_params), matvec=G2_as_operator)\n",
    "    else:\n",
    "        print(\"Wrong G_type\")\n",
    "\n",
    "\n",
    "    np_RHS_vec = tensor_to_numpy(RHS_vec)\n",
    "    sol_vec, info = scipy.sparse.linalg.minres(G_operator, np_RHS_vec, rtol=minres_tolerance)\n",
    "    if (torch.max(torch.isnan(torch.tensor(sol_vec))) > 0):\n",
    "        print(\"Got the NAN!!!\")\n",
    "        sol_vec = np_RHS_vec\n",
    "        info = 0\n",
    "    tensorized_sol_vec = torch.Tensor(sol_vec).to(device)\n",
    "\n",
    "    return tensorized_sol_vec, info #, norm_err_vec\n",
    "\n",
    "\n",
    "def minres_solver(net, net_auxil, interior_samples, boundary_samples, RHS_vec, device, bd_lambda, max_iternum, minres_tolerance):\n",
    "\n",
    "    def G_as_operator(vec):  # input the vector v [on CPU], return vector Gv\n",
    "        tensorized_vec = torch.Tensor(vec).to(device)\n",
    "        Gv = metric_tensor_as_Laplace_op(net, net_auxil, interior_samples, tensorized_vec, device) + bd_lambda * metric_tensor_as_trace_op(net, net_auxil, boundary_samples, tensorized_vec, device)\n",
    "        return tensor_to_numpy(Gv)  # return a numpy array on CPU\n",
    "\n",
    "    num_params = torch.nn.utils.parameters_to_vector(net.parameters()).size()[0]\n",
    "    G_operator = LinearOperator((num_params, num_params), matvec=G_as_operator)\n",
    "\n",
    "    np_RHS_vec = tensor_to_numpy(RHS_vec)\n",
    "    np_RHS_vec_copy = np.copy(np_RHS_vec)\n",
    "    sol_vec, info = scipy.sparse.linalg.minres(G_operator, np_RHS_vec, rtol=minres_tolerance)\n",
    "    if (torch.max(torch.isnan(torch.tensor(sol_vec))) > 0):\n",
    "        print(\"Got the NAN!!!\")\n",
    "        sol_vec = np_RHS_vec\n",
    "        info = 0\n",
    "    tensorized_sol_vec = torch.Tensor(sol_vec).to(device)\n",
    "\n",
    "    return tensorized_sol_vec, info #, norm_err_vec\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "5kyfJ8Mpd7w7"
   },
   "outputs": [],
   "source": [
    "# @title update param via line search\n",
    "\n",
    "def update_param(number_stepsizes, stepsize_0, base_stepsize, theta_0, tangent_theta, net_T_or_u, hidden_dim_net_T_or_u, net_phi, hidden_dim_net_phi, rho0_samples, rho1_samples, lossfunction, net_type, descent_or_ascent):\n",
    "\n",
    "    stepsize_list = stepsize_0 * base_stepsize**np.arange(number_stepsizes)\n",
    "    min_index = 0\n",
    "    index = 0\n",
    "    loss_along_stepsizes = []\n",
    "    current_min = 1000\n",
    "\n",
    "    if descent_or_ascent == \"descent\":\n",
    "        flag = -1\n",
    "    else:\n",
    "        flag = 1\n",
    "\n",
    "\n",
    "    if net_type == \"T\":\n",
    "        net_test = network_prim(network_length, dim, hidden_dim_net_T_or_u, dim).to(device)\n",
    "    elif net_type == \"u\":\n",
    "        net_test = network_prim(network_length, dim, hidden_dim_net_T_or_u, 1).to(device)\n",
    "    elif net_type == \"phi\":\n",
    "        net_test = network_dual(network_length, dim, hidden_dim_net_phi, 1).to(device)\n",
    "    else:\n",
    "        print(\"Wrong net_type\")\n",
    "\n",
    "    for stepsize in stepsize_list:\n",
    "        updated_theta = theta_0 + flag * stepsize * tangent_theta\n",
    "        torch.nn.utils.vector_to_parameters(updated_theta, net_test.parameters())\n",
    "        if net_type == \"T\" or net_type == \"u\":\n",
    "            loss = lossfunction(net_test, net_phi, rho0_samples, rho1_samples)\n",
    "        elif net_type == \"phi\":\n",
    "            loss = lossfunction(net_T_or_u, net_test, rho0_samples, rho1_samples)\n",
    "        else:\n",
    "            print(\"Wrong net_type\")\n",
    "        loss_along_stepsizes.append( - flag * loss.cpu().detach().numpy() )\n",
    "        if loss < current_min:\n",
    "            min_index = index\n",
    "            current_min = loss\n",
    "\n",
    "        index = index + 1\n",
    "\n",
    "    optimal_stepsize = stepsize_list[min_index]\n",
    "    optimal_updated_theta = theta_0 + flag * optimal_stepsize * tangent_theta\n",
    "\n",
    "    return optimal_updated_theta, optimal_stepsize, loss_along_stepsizes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "j_eB-c838Ulz"
   },
   "outputs": [],
   "source": [
    "# @title PD Adam solver use T instead of grad u\n",
    "\n",
    "n_bins = 400\n",
    "\n",
    "def PD_Adam_solver_with_T( device, save_path, L, N,\n",
    "                    network_length, hidden_dimension_net_T, hidden_dimension_net_phi, flag_init,\n",
    "                    lr_T, lr_phi, iter, phi_iter, T_iter,\n",
    "                    plot_period, N_plot, chosen_dim_0, chosen_dim_1, chosen_dim_2, chosen_dim_3):\n",
    "\n",
    "    torch.manual_seed(50)\n",
    "\n",
    "    # initialize nets\n",
    "    net_T = network_prim(network_length, dim, hidden_dimension_net_T, dim).to(device)\n",
    "    optim_T = torch.optim.Adam(net_T.parameters(), lr=lr_T)\n",
    "    net_phi = network_dual(network_length, dim, hidden_dimension_net_phi, 1).to(device)\n",
    "    optim_phi = torch.optim.Adam(net_phi.parameters(), lr=lr_phi)\n",
    "    if flag_init == True:\n",
    "        net_T.initialization()\n",
    "        net_phi.initialization()\n",
    "\n",
    "    # initial err\n",
    "    error_0 = L2_error_T_with_OT_map_inv(net_T, 1800)\n",
    "    print(\"initial error = {}\".format(error_0.cpu().detach().numpy()))\n",
    "\n",
    "    # loss = PDHG_loss3\n",
    "    loss = PDHG_loss4   #  use L3 loss with quadratic term\n",
    "\n",
    "    rho0_samples = rho0(N)\n",
    "    plt_histogram_pushfwd_rho0_rho1_densitycurve(0, L, N_plot, net_T, n_bins)\n",
    "    One_D_plot_T_with_real_solution(net_T, L, 400, 0, 1, save_path, device )\n",
    "\n",
    "    comp_time = []\n",
    "    total_time = 0.0\n",
    "    l2error_list = []\n",
    "    ######################################################### PDHG iterations START HERE ###################################################################################################################\n",
    "    for t in range(iter):\n",
    "\n",
    "        t_0 = time.time()\n",
    "\n",
    "        rho0_samples = rho0(N)\n",
    "        rho1_samples = rho1(N)\n",
    "\n",
    "        # net_u.zero_grad()\n",
    "        # net_phi.zero_grad()\n",
    "        ############################# update phi_\\eta #####################################\n",
    "        for inner_iter in range(phi_iter):\n",
    "            optim_phi.zero_grad()\n",
    "            lossa = - loss(net_T, net_phi,  rho0_samples, rho1_samples)\n",
    "            lossa.backward(retain_graph=True)\n",
    "            optim_phi.step()\n",
    "\n",
    "        ######################## update theta ##################################\n",
    "        for inner_iter in range(T_iter):\n",
    "            optim_T.zero_grad()\n",
    "            lossb = loss(net_T, net_phi, rho0_samples, rho1_samples)\n",
    "            lossb.backward(retain_graph=True)\n",
    "            optim_T.step()\n",
    "\n",
    "        t_1 = time.time()\n",
    "        total_time = total_time + (t_1 - t_0)\n",
    "        comp_time.append(total_time)\n",
    "\n",
    "        ################ plot ##################\n",
    "        if (t+1) % plot_period == 0:\n",
    "            plt_histogram_pushfwd_rho0_rho1_densitycurve(t, L, N_plot, net_T, n_bins)\n",
    "            One_D_plot_T_with_real_solution(net_T, L, 400, t, 1, save_path, device )\n",
    "\n",
    "        ##############\n",
    "        print(\"Iter: {}, \".format(t))\n",
    "        L2error = L2_error_T_with_OT_map_inv(net_T, 1800)\n",
    "        l2error_list.append(L2error.cpu().detach())\n",
    "        print(\"L2 error = {}\".format(L2error))\n",
    "\n",
    "    ######################################################### PDHG iterations END HERE #################################################################################################################\n",
    "    # save the models\n",
    "    save_path = os.getcwd()\n",
    "    filename = os.path.join(save_path, 'netT_PD_Adam.pt')\n",
    "    torch.save(net_T.state_dict(), filename)\n",
    "\n",
    "    save_path = os.getcwd()\n",
    "    filename = os.path.join(save_path, 'netphi_PD_Adam.pt')\n",
    "    torch.save(net_phi.state_dict(), filename)\n",
    "\n",
    "    # write down the error\n",
    "    with open('l2error_list', 'wb') as file1:\n",
    "        pickle.dump(l2error_list, file1)\n",
    "    with open('comp_time', 'wb') as f:\n",
    "        pickle.dump(comp_time, f)\n",
    "\n",
    "    # plot the error decay\n",
    "    fig_plot = plt.figure(figsize=(20, 20))\n",
    "    plt.plot(range(0, len(l2error_list)), log(l2error_list)/log(10))\n",
    "    plt.title(\"plot of log_10(L2 error)\")\n",
    "    fig_plot.savefig(\"Plot of the log l2 error\"+'.pdf')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # plot the error decay\n",
    "    fig_plot = plt.figure(figsize=(20, 20))\n",
    "    plt.plot(comp_time, log(l2error_list)/log(10))\n",
    "    plt.title(\"log_10(L2 error) vs. computation time (seconds)\\n\", fontsize=40)\n",
    "    plt.xlabel(\"computation time (seconds)\", fontsize=30)\n",
    "    plt.ylabel(\"log_10(L2 error)\", fontsize=30)\n",
    "    fig_plot.savefig(\"Plot of the log l2 error vs comp time \"+'.pdf')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "collapsed": true,
    "id": "I1UBAe2qPXQ_"
   },
   "outputs": [],
   "source": [
    "# @title apply PD_Adam solver\n",
    "save_path = os.getcwd()\n",
    "\n",
    "\n",
    "# In Google Colab, change to working directory\n",
    "os.chdir(\"/content/drive/MyDrive/NPDG_codes_collection/OT1D\")\n",
    "# Verify\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# create folder for NPDHG\n",
    "os.makedirs('PD Adam', exist_ok=True)\n",
    "os.chdir('PD Adam')\n",
    "\n",
    "\n",
    "L = 4.5\n",
    "N = 800\n",
    "\n",
    "iter = 1 # 40000\n",
    "phi_iter = 1\n",
    "T_iter = 1\n",
    "lr_T = 5 *1e-4\n",
    "lr_phi = 5*1e-4\n",
    "\n",
    "network_length =  3\n",
    "hidden_dimension_net_T = 50\n",
    "hidden_dimension_net_phi = 50\n",
    "flag_init = False\n",
    "\n",
    "plot_period = 1 # 2000\n",
    "N_plot = 6500\n",
    "chosen_dim_0 = 0\n",
    "chosen_dim_1 = 1\n",
    "chosen_dim_2 = 3\n",
    "chosen_dim_3 = 4\n",
    "PD_Adam_solver_with_T( device, save_path, L, N,\n",
    "                    network_length, hidden_dimension_net_T, hidden_dimension_net_phi, flag_init,\n",
    "                    lr_T, lr_phi, iter, phi_iter, T_iter,\n",
    "                    plot_period, N_plot, chosen_dim_0, chosen_dim_1, chosen_dim_2, chosen_dim_3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "RYGBqYM8p56s"
   },
   "outputs": [],
   "source": [
    "# @title NPDHG solver\n",
    "import pickle\n",
    "\n",
    "\n",
    "n_bins = 400\n",
    "def NPDHG_solver(device, save_path, L, N,\n",
    "                minres_max_iter, minres_tol,\n",
    "                network_length, hidden_dimension_net_T, hidden_dimension_net_phi, flag_init,\n",
    "                iter, phi_iter, T_iter, omega,\n",
    "                plot_period, print_period, N_plot,\n",
    "                number_stepsizes, base_stepsize,\n",
    "                precond_type,\n",
    "                stepsize_0=0.2,\n",
    "                adaptive_or_fixed_stepsize=\"fixed\", tau_T = 0.5 * 1e-3, tau_phi = 0.9 * 1e-3 ):\n",
    "\n",
    "    torch.manual_seed(50)\n",
    "\n",
    "    # initialize nets\n",
    "    net_T = network_prim(network_length, dim, hidden_dimension_net_T, dim).to(device)\n",
    "    net_phi = network_dual(network_length, dim, hidden_dimension_net_phi, 1).to(device)\n",
    "\n",
    "    if flag_init == True:\n",
    "        net_T.initialization()\n",
    "        net_phi.initialization()\n",
    "\n",
    "    # initial err\n",
    "    error_0 = L2_error_T_with_OT_map_inv(net_T, 2000)\n",
    "    print(error_0)\n",
    "\n",
    "    if precond_type == \"MpMd_id\":\n",
    "       G_T_type = \"1\"\n",
    "       G_phi_type = \"1\"\n",
    "    elif precond_type == \"Mp_id_Md_nabla\":\n",
    "       G_T_type = \"1\"\n",
    "       G_phi_type = \"2\"\n",
    "    else:\n",
    "       print(\"Wrong precond_type\")\n",
    "    loss = PDHG_loss4\n",
    "\n",
    "    rho0_samples = rho0(N)\n",
    "    plt_histogram_pushfwd_rho0_rho1_densitycurve(0, L, N_plot, net_T, n_bins)\n",
    "    One_D_plot_T_with_real_solution(net_T, L, 400, 0, 1, save_path, device )\n",
    "\n",
    "    total_time = 0.0\n",
    "    comptime = []\n",
    "    l2error_list = []\n",
    "    preconded_nabla_eta_norm_list = []\n",
    "    preconded_nabla_theta_norm_list = []\n",
    "    ######################################################### PDHG iterations START HERE ###################################################################################################################\n",
    "    for t in range(iter):\n",
    "\n",
    "        t_0 = time.time()\n",
    "\n",
    "        net_phi_0 = network_dual(network_length, dim, hidden_dimension_net_phi, 1).to(device)\n",
    "        net_phi_0.load_state_dict(net_phi.state_dict())\n",
    "        rho0_samples = rho0(N)\n",
    "        rho1_samples = rho1(N)\n",
    "\n",
    "        net_T.zero_grad()\n",
    "        net_phi.zero_grad()\n",
    "        ############################# update phi_\\eta #####################################\n",
    "        for inner_iter in range(phi_iter):\n",
    "            ############### compute G(\\eta)^{-1} \\nabla_\\eta loss() #########################\n",
    "            lossa = loss(net_T, net_phi,  rho0_samples, rho1_samples)\n",
    "            nabla_eta_loss = torch.autograd.grad(lossa, net_phi.parameters(), grad_outputs=None, allow_unused=True, retain_graph=True, create_graph=True)\n",
    "            vectorized_nabla_eta_loss = torch.nn.utils.parameters_to_vector(nabla_eta_loss)\n",
    "\n",
    "            # copy net_phi for G(\\eta) computation\n",
    "            net_phi_auxil = network_dual(network_length, dim, hidden_dimension_net_phi, 1).to(device)\n",
    "            net_phi_auxil.load_state_dict(net_phi.state_dict())\n",
    "\n",
    "            # compute G(\\eta)^{-1} \\nabla_\\eta loss()\n",
    "            G_inv_nabla_eta_loss, info_phi = minres_solver_G(net_phi, net_phi_auxil, rho1_samples, None, vectorized_nabla_eta_loss, device, None, minres_max_iter, minres_tol, G_phi_type)\n",
    "\n",
    "            # update \\eta\n",
    "            original_eta = torch.nn.utils.parameters_to_vector(net_phi.parameters())\n",
    "            if adaptive_or_fixed_stepsize == \"adaptive\":\n",
    "                updated_eta, tau_phi, value_along_tau_phis = update_param(number_stepsizes, stepsize_0, base_stepsize, original_eta, G_inv_nabla_eta_loss, net_T, hidden_dimension_net_T, net_phi, hidden_dimension_net_phi, rho0_samples, rho1_samples, loss, \"phi\", \"ascent\")\n",
    "            elif adaptive_or_fixed_stepsize == \"fixed\":\n",
    "                updated_eta = original_eta + tau_phi * G_inv_nabla_eta_loss\n",
    "            else:\n",
    "                raise ValueError(\"adaptive_or_fixed_stepsize must be 'adaptive' or 'fixed'\")\n",
    "            torch.nn.utils.vector_to_parameters(updated_eta, net_phi.parameters())\n",
    "\n",
    "        ######################## update theta ##################################\n",
    "        for inner_iter in range(T_iter):\n",
    "            # compute G(\\theta)^{-1} \\nabla_\\theta loss()\n",
    "            lossb = PDHG_loss4_with_extrapolation(net_T, net_phi, net_phi_0, rho0_samples, rho1_samples, omega)\n",
    "            nabla_theta_loss = torch.autograd.grad(lossb, net_T.parameters(), grad_outputs=None, allow_unused=True, retain_graph=True, create_graph=True)\n",
    "            vectorized_nabla_theta_loss = torch.nn.utils.parameters_to_vector(nabla_theta_loss)\n",
    "\n",
    "            # copy net_T for  G(\\theta) computation\n",
    "            net_T_auxil = network_prim(network_length, dim, hidden_dimension_net_T, dim).to(device)\n",
    "            net_T_auxil.load_state_dict(net_T.state_dict())\n",
    "\n",
    "            # compute G(\\theta)^{-1} \\nabla_\\theta loss()\n",
    "            G_inv_nabla_theta_loss, info_T = minres_solver_G(net_T, net_T_auxil, rho0_samples, None, vectorized_nabla_theta_loss, device, None, minres_max_iter, minres_tol, G_T_type)\n",
    "\n",
    "            ############# update theta ####################\n",
    "            original_theta = torch.nn.utils.parameters_to_vector(net_T.parameters())\n",
    "            if adaptive_or_fixed_stepsize == \"adaptive\":\n",
    "               updated_theta, tau_T, value_along_tau_Ts = update_param(number_stepsizes, stepsize_0, base_stepsize, original_theta, G_inv_nabla_theta_loss, net_T, hidden_dimension_net_T, net_phi, hidden_dimension_net_phi, rho0_samples, rho1_samples, loss, \"T\", \"descent\")\n",
    "            elif adaptive_or_fixed_stepsize == \"fixed\":\n",
    "               updated_theta = original_theta - tau_T * G_inv_nabla_theta_loss\n",
    "            else:\n",
    "               raise ValueError(\"adaptive_or_fixed_stepsize must be 'adaptive' or 'fixed'\")\n",
    "            torch.nn.utils.vector_to_parameters(updated_theta, net_T.parameters())\n",
    "\n",
    "        t_1 = time.time()\n",
    "        total_time += t_1 - t_0\n",
    "        comptime.append(total_time)\n",
    "\n",
    "        ################ plot ##################\n",
    "        if (t+1) % plot_period == 0:\n",
    "            plt_histogram_pushfwd_rho0_rho1_densitycurve(t, L, N_plot, net_T, n_bins)\n",
    "            One_D_plot_T_with_real_solution(net_T, L, 400, t, 1, save_path, device)\n",
    "\n",
    "        ########################################################\n",
    "        L2error = L2_error_T_with_OT_map_inv(net_T, 1800)\n",
    "        l2error_list.append(L2error.cpu().detach())\n",
    "        if (t+1) % print_period == 0:\n",
    "            print(\"Iteration: {}\".format(t))\n",
    "            print(\"L2 error = {}\".format(L2error))\n",
    "\n",
    "\n",
    "    ######################################################### PDHG iterations END HERE #################################################################################################################\n",
    "    # save the models\n",
    "    save_path = os.getcwd()\n",
    "    filename = os.path.join(save_path, 'netT_NPDHG.pt')\n",
    "    torch.save(net_T.state_dict(), filename)\n",
    "\n",
    "    save_path = os.getcwd()\n",
    "    filename = os.path.join(save_path, 'netphi_NPDHG.pt')\n",
    "    torch.save(net_phi.state_dict(), filename)\n",
    "\n",
    "    # write down the error\n",
    "    with open('l2error_list', 'wb') as file1:\n",
    "        pickle.dump(l2error_list, file1)\n",
    "    with open('comptime', 'wb') as f:\n",
    "        pickle.dump(comptime, f)\n",
    "\n",
    "    # plot the error decay\n",
    "    fig_plot = plt.figure(figsize=(20, 20))\n",
    "    plt.plot(range(0, len(l2error_list)), log(l2error_list)/log(10))\n",
    "    plt.title(\"plot of log_10(L2 error)\")\n",
    "    fig_plot.savefig(\"Plot of the log l2 error\"+'.pdf')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # plot the error decay\n",
    "    fig_plot = plt.figure(figsize=(20, 20))\n",
    "    plt.plot(comptime, log(l2error_list)/log(10))\n",
    "    plt.title(\"log_10(L2 error) vs. computation time (seconds)\\n\", fontsize=40)\n",
    "    plt.xlabel(\"computation time (seconds)\", fontsize=30)\n",
    "    plt.ylabel(\"log_10(L2 error)\", fontsize=30)\n",
    "    fig_plot.savefig(\"Plot of the log l2 error vs comp time \"+'.pdf')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "collapsed": true,
    "id": "DLL2h8DKNtHO"
   },
   "outputs": [],
   "source": [
    "# @title apply NPDHG solver (use M_d = Id in preconditioner)\n",
    "save_path = os.getcwd()\n",
    "\n",
    "\n",
    "# In Google Colab, change to working directory\n",
    "os.chdir(\"/content/drive/MyDrive/NPDG_codes_collection/OT1D\")\n",
    "# Verify\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# create folder for NPDHG\n",
    "os.makedirs('NPDHG precondition1', exist_ok=True)\n",
    "os.chdir('NPDHG precondition1')\n",
    "\n",
    "\n",
    "L = 4.5\n",
    "N = 800\n",
    "\n",
    "minres_max_iter = 1000\n",
    "minres_tol = 1e-3\n",
    "\n",
    "network_length = 3\n",
    "hidden_dimension_net_T = 50\n",
    "hidden_dimension_net_phi = 50\n",
    "flag_init = False\n",
    "\n",
    "iter = 6000\n",
    "phi_iter = 1\n",
    "T_iter = 1\n",
    "omega = 1.0\n",
    "\n",
    "plot_period = 1000\n",
    "print_period = 200\n",
    "N_plot = 5000\n",
    "\n",
    "number_stepsizes = 50\n",
    "base_stepsize = 0.8\n",
    "\n",
    "precond_type = \"MpMd_id\"\n",
    "\n",
    "NPDHG_solver(device, save_path, L, N,\n",
    "            minres_max_iter, minres_tol,\n",
    "            network_length, hidden_dimension_net_T, hidden_dimension_net_phi, flag_init,\n",
    "            iter, phi_iter, T_iter, omega,\n",
    "            plot_period, print_period, N_plot,\n",
    "            number_stepsizes, base_stepsize,\n",
    "            precond_type,\n",
    "            adaptive_or_fixed_stepsize=\"fixed\", tau_T = 1e-1, tau_phi = 1.5 * 1e-1\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "yIYZMuPpqTPq"
   },
   "outputs": [],
   "source": [
    "# @title apply NPDHG solver (use M_d = \\nabla in preconditioner)\n",
    "save_path = os.getcwd()\n",
    "\n",
    "\n",
    "# In Google Colab, change to working directory\n",
    "os.chdir(\"/content/drive/MyDrive/NPDG_codes_collection/OT1D\")\n",
    "# Verify\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# create folder for NPDHG\n",
    "os.makedirs('NPDHG precondition2', exist_ok=True)\n",
    "os.chdir('NPDHG precondition2')\n",
    "\n",
    "\n",
    "L = 4.5\n",
    "N = 800\n",
    "\n",
    "minres_max_iter = 1000\n",
    "minres_tol = 1e-3\n",
    "\n",
    "network_length = 3\n",
    "hidden_dimension_net_T = 50\n",
    "hidden_dimension_net_phi = 50\n",
    "flag_init = False\n",
    "\n",
    "iter = 6000\n",
    "phi_iter = 1\n",
    "T_iter = 1\n",
    "omega = 1.0\n",
    "\n",
    "plot_period =  1000\n",
    "print_period =  200\n",
    "N_plot = 5000\n",
    "\n",
    "number_stepsizes = 50\n",
    "base_stepsize = 0.8\n",
    "\n",
    "precond_type = \"Mp_id_Md_nabla\"\n",
    "\n",
    "NPDHG_solver(device, save_path, L, N,\n",
    "            minres_max_iter, minres_tol,\n",
    "            network_length, hidden_dimension_net_T, hidden_dimension_net_phi, flag_init,\n",
    "            iter, phi_iter, T_iter, omega,\n",
    "            plot_period, print_period, N_plot,\n",
    "            number_stepsizes, base_stepsize,\n",
    "            precond_type,\n",
    "            adaptive_or_fixed_stepsize=\"fixed\", tau_T = 1e-1, tau_phi = 1.5 * 1e-1\n",
    "            )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
