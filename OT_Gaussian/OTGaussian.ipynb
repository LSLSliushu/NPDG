{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSYM5-A-QTlm",
        "outputId": "65fdfb3f-31aa-4e23-b43a-b3ebad2bb65e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FHjjp6gI1nO"
      },
      "outputs": [],
      "source": [
        "cd/content/drive/MyDrive/NPDG/OT_Gaussian"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook computes the optimal transport (OT) map between Gaussian distributions using\n",
        "\n",
        "\n",
        "*   NPDG method\n",
        "*   Primal-Dual Adam method\n",
        "\n",
        "See Section 5.5.2 for more detailed description of the problem.\n",
        "\n"
      ],
      "metadata": {
        "id": "fyZ5ZJgLDxx4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QSjLeuOZRg0t"
      },
      "outputs": [],
      "source": [
        "# @title import\n",
        "import math\n",
        "import random\n",
        "import scipy\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('agg')\n",
        "\n",
        "from matplotlib.pyplot import figure\n",
        "from mpl_toolkits import mplot3d\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from torch.func import grad, hessian, vmap\n",
        "from torch.func import jacrev\n",
        "from torch.func import functional_call\n",
        "import time\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import pickle\n",
        "\n",
        "from numpy import *\n",
        "from torch import Tensor\n",
        "from torch.nn import Parameter\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OlXCS21tnzEz"
      },
      "outputs": [],
      "source": [
        "# @title set dimension of the problem\n",
        "\n",
        "dim = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QqmvN4WRcGEy"
      },
      "outputs": [],
      "source": [
        "# @title Check CUDA availability\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-HZo9kPmpqk_"
      },
      "outputs": [],
      "source": [
        "# @title model (net)\n",
        "\n",
        "\n",
        "class network_map(nn.Module):\n",
        "    def __init__(self, network_length, input_dimension, hidden_dimension, output_dimension):\n",
        "        super(network_prim, self).__init__()\n",
        "\n",
        "        self.network_length = network_length\n",
        "        self.linears = nn.ModuleList([nn.Linear(input_dimension, hidden_dimension)])\n",
        "        self.linears.extend([nn.Linear(hidden_dimension, hidden_dimension) for _ in range(1, network_length-1)])\n",
        "        self.linears.extend([nn.Linear(hidden_dimension, output_dimension, bias=False)])\n",
        "        self.softplus = nn.Softplus()\n",
        "\n",
        "    def initialization(self):\n",
        "        for l in self.linears:\n",
        "            l.weight.data.normal_()\n",
        "            if l.bias is not None:\n",
        "                l.bias.data.normal_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        l = self.linears[0]\n",
        "        x = l(x)\n",
        "        for l in self.linears[1: self.network_length-1]:\n",
        "            x = self.softplus(x)\n",
        "            x = l(x)\n",
        "\n",
        "        x = self.softplus(x)\n",
        "        l = self.linears[self.network_length-1]\n",
        "        x = l(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class network_prim(nn.Module):\n",
        "    def __init__(self, network_length, input_dimension, hidden_dimension, output_dimension):\n",
        "        super(network_prim, self).__init__()\n",
        "\n",
        "        self.network_length = network_length\n",
        "        self.linears = nn.ModuleList([nn.Linear(input_dimension, hidden_dimension)])\n",
        "        self.linears.extend([nn.Linear(hidden_dimension, hidden_dimension) for _ in range(1, network_length-1)])\n",
        "        self.linears.extend([nn.Linear(hidden_dimension, output_dimension, bias=False)])\n",
        "        self.softplus = nn.Softplus()\n",
        "\n",
        "    def initialization(self):\n",
        "        for l in self.linears:\n",
        "            l.weight.data.normal_()\n",
        "            if l.bias is not None:\n",
        "                l.bias.data.normal_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        l = self.linears[0]\n",
        "        x = l(x)\n",
        "        for l in self.linears[1: self.network_length-1]:\n",
        "            x = self.softplus(x)\n",
        "            x = l(x)\n",
        "        x = self.softplus(x)\n",
        "        l = self.linears[self.network_length-1]\n",
        "        x = l(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class network_dual(nn.Module):\n",
        "    def __init__(self, network_length, input_dimension, hidden_dimension, output_dimension):\n",
        "        super(network_dual, self).__init__()\n",
        "\n",
        "        self.network_length = network_length\n",
        "        self.linears = nn.ModuleList([nn.Linear(input_dimension, hidden_dimension)])\n",
        "        self.linears.extend([nn.Linear(hidden_dimension, hidden_dimension) for _ in range(1, network_length-1)])\n",
        "        self.linears.extend([nn.Linear(hidden_dimension, output_dimension, bias=False)])\n",
        "        self.softplus = nn.Softplus()\n",
        "\n",
        "    def initialization(self):\n",
        "        for l in self.linears:\n",
        "            l.weight.data.normal_()\n",
        "            if l.bias is not None:\n",
        "                l.bias.data.normal_()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        l = self.linears[0]\n",
        "        x = l(x)\n",
        "        for l in self.linears[1: self.network_length-1]:\n",
        "            x = self.softplus(x)\n",
        "            x = l(x)\n",
        "\n",
        "        x = self.softplus(x)\n",
        "        l = self.linears[self.network_length-1]\n",
        "        x = l(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RfVV1ZHO8IHr"
      },
      "outputs": [],
      "source": [
        "# @title 2D vector field plotting & 2D pushforwarded sample\n",
        "\n",
        "def plot_grad_u_with_grad_ureal(iter, samples, l, chosen_dim_0, chosen_dim_1, net_u, save_path):\n",
        "\n",
        "    grad_netu = 3. * gradient_nn(net_u, samples)\n",
        "    grad_netu = grad_netu.cpu().detach().numpy()\n",
        "    grad_ureal = 3. * grad_u_real(samples)\n",
        "    grad_ureal = grad_ureal.cpu().detach().numpy()\n",
        "\n",
        "    samples = samples.cpu().detach().numpy()\n",
        "\n",
        "    figure(num=None, figsize=(34, 34), dpi=80, facecolor='w', edgecolor='k')\n",
        "    plt.xlim([-l, l])\n",
        "    plt.ylim([-l, l])\n",
        "    plt.quiver(samples[:, chosen_dim_0], samples[:, chosen_dim_1], grad_netu[:, chosen_dim_0], grad_netu[:, chosen_dim_1], scale=None, scale_units='inches', color = 'green', width=0.002)\n",
        "    plt.quiver(samples[:, chosen_dim_0], samples[:, chosen_dim_1], grad_ureal[:, chosen_dim_0], grad_ureal[:, chosen_dim_1], scale=None, scale_units='inches', color = 'red', width=0.002)\n",
        "    plt.xlabel(\"{} component\".format(chosen_dim_0))\n",
        "    plt.ylabel(\"{} component\".format(chosen_dim_1))\n",
        "\n",
        "    filename = os.path.join(save_path, '(Iteration={}) gradient of net_u with gradient of u_real at sample points (on {}-{} plane)'.format(iter, chosen_dim_0, chosen_dim_1) + '.pdf')\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_pushfwded_samples(iter, net_T, samples, chosen_dim_0, chosen_dim_1, save_path):\n",
        "\n",
        "    N = samples.size()[0]\n",
        "\n",
        "    T_x = net_T(samples)\n",
        "    T_x = T_x.cpu().detach().numpy()\n",
        "    samples = samples.cpu().detach().numpy()\n",
        "    target_samples = rho1(N).cpu()\n",
        "\n",
        "    figure(num=None, figsize=(34, 34), dpi=80, facecolor='w', edgecolor='k')\n",
        "    plt.scatter(samples[:, 0], samples[:, 1], color='green', s=9)\n",
        "    plt.scatter(T_x[:, 0], T_x[:, 1], color='blue', s=9)\n",
        "    plt.scatter(target_samples[:, 0], target_samples[:, 1], color='red', s=9, alpha=0.4)\n",
        "    plt.xlim([-3, 3])\n",
        "    plt.ylim([-3, 3])\n",
        "    plt.xlabel(\"{} component\".format(chosen_dim_0))\n",
        "    plt.ylabel(\"{} component\".format(chosen_dim_1))\n",
        "\n",
        "    filename = os.path.join(save_path, '(Iteration={}) original samples (green, ~rho0) pushforwarded samples (blue) with target samples (red ~rho1) (on {}-{} plane)'.format(iter, chosen_dim_0, chosen_dim_1) + '.pdf')\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_T_with_grad_ureal(iter, samples, l, chosen_dim_0, chosen_dim_1, net_T, save_path):\n",
        "\n",
        "    T_x = net_T(samples)\n",
        "    T_x = T_x.cpu().detach().numpy()\n",
        "    grad_ureal = grad_u_real(samples)\n",
        "    grad_ureal = grad_ureal.cpu().detach().numpy()\n",
        "\n",
        "    samples = samples.cpu().detach().numpy()\n",
        "\n",
        "    figure(num=None, figsize=(14, 14), dpi=80, facecolor='w', edgecolor='k')\n",
        "    plt.xlim([-l, l])\n",
        "    plt.ylim([-l, l])\n",
        "    q = plt.quiver(samples[:, chosen_dim_0], samples[:, chosen_dim_1], grad_ureal[:, chosen_dim_0], grad_ureal[:, chosen_dim_1], scale = 1.6, scale_units='inches', color = 'red', width=0.0024, label='real OT map')\n",
        "    plt.quiver(samples[:, chosen_dim_0], samples[:, chosen_dim_1], T_x[:, chosen_dim_0], T_x[:, chosen_dim_1], scale = q.scale, scale_units='inches', color=(1.6/100, 46.3/100, 81.6/100), width=0.001, label='OT map computed via NPDG')\n",
        "    plt.tick_params(axis='both', labelsize=30)\n",
        "    plt.legend(fontsize = 36)\n",
        "    plt.xlabel(\"x_{}\".format(chosen_dim_0+1), fontsize = 30)\n",
        "    plt.ylabel(\"x_{}\".format(chosen_dim_1+1), fontsize = 30)\n",
        "    filename = os.path.join(save_path, '[a]Map T with gradient of u_real at sample points (on {}-{} plane)'.format(iter, chosen_dim_0, chosen_dim_1) + '.pdf')\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_T_with_grad_ureal_two_plots(iter, samples, l, chosen_dim_0, chosen_dim_1, chosen_dim_2, chosen_dim_3, net_T, save_path):\n",
        "\n",
        "    T_x = net_T(samples)\n",
        "    T_x = T_x.cpu().detach().numpy()\n",
        "    grad_ureal = grad_u_real(samples)\n",
        "    grad_ureal = grad_ureal.cpu().detach().numpy()\n",
        "\n",
        "    samples = samples.cpu().detach().numpy()\n",
        "\n",
        "    plt.subplot(2, 1, 1)\n",
        "    figure(num=None, figsize=(27, 27), dpi=80, facecolor='w', edgecolor='k')\n",
        "    plt.xlim([-l, l])\n",
        "    plt.ylim([-l, l])\n",
        "    q = plt.quiver(samples[:, chosen_dim_0], samples[:, chosen_dim_1], grad_ureal[:, chosen_dim_0], grad_ureal[:, chosen_dim_1], scale = 1, scale_units='inches', color = 'red', width=0.0024, label='real map')\n",
        "    plt.quiver(samples[:, chosen_dim_0], samples[:, chosen_dim_1], T_x[:, chosen_dim_0], T_x[:, chosen_dim_1], scale = q.scale, scale_units='inches', color=(1.6/100, 46.3/100, 81.6/100), width=0.001, label='NPDG')\n",
        "    plt.xlabel(\"x_{}\".format(chosen_dim_0+1), fontsize = 30)\n",
        "    plt.ylabel(\"x_{}\".format(chosen_dim_1+1), fontsize = 30)\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    figure(num=None, figsize=(27, 27), dpi=80, facecolor='w', edgecolor='k')\n",
        "    plt.xlim([-l, l])\n",
        "    plt.ylim([-l, l])\n",
        "    q = plt.quiver(samples[:, chosen_dim_2], samples[:, chosen_dim_3], grad_ureal[:, chosen_dim_2], grad_ureal[:, chosen_dim_3], scale = 1, scale_units='inches', color = 'red', width=0.0024, label='real map')\n",
        "    plt.quiver(samples[:, chosen_dim_2], samples[:, chosen_dim_3], T_x[:, chosen_dim_2], T_x[:, chosen_dim_3], scale = q.scale, scale_units='inches', color=(1.6/100, 46.3/100, 81.6/100), width=0.001, label='NPDG')\n",
        "    plt.xlabel(\"x_{}\".format(chosen_dim_2+1), fontsize = 30)\n",
        "    plt.ylabel(\"x_{}\".format(chosen_dim_3+1), fontsize = 30)\n",
        "\n",
        "    filename = os.path.join(save_path, 'Map T with gradient of u_real at sample points '  + '.pdf')\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_pushfwded_samples(iter, net_T, samples, chosen_dim_0, chosen_dim_1, save_path):\n",
        "\n",
        "    N = samples.size()[0]\n",
        "\n",
        "    T_x = net_T(samples)\n",
        "    T_x = T_x.cpu().detach().numpy()\n",
        "    samples = samples.cpu().detach().numpy()\n",
        "    target_samples = rho1(N).cpu()\n",
        "\n",
        "    figure(num=None, figsize=(34, 34), dpi=80, facecolor='w', edgecolor='k')\n",
        "    plt.scatter(samples[:, 0], samples[:, 1], color='green', s=9)\n",
        "    plt.scatter(T_x[:, 0], T_x[:, 1], color='blue', s=9)\n",
        "    plt.scatter(target_samples[:, 0], target_samples[:, 1], color='red', s=9, alpha=0.4)\n",
        "    plt.xlim([-3, 3])\n",
        "    plt.ylim([-3, 3])\n",
        "    plt.xlabel(\"{} component\".format(chosen_dim_0))\n",
        "    plt.ylabel(\"{} component\".format(chosen_dim_1))\n",
        "\n",
        "    filename = os.path.join(save_path, '(Iteration={}) original samples (green, ~rho0) pushforwarded samples (blue) with target samples (red ~rho1) (on {}-{} plane)'.format(iter, chosen_dim_0, chosen_dim_1) + '.pdf')\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ukl93MWUML1H"
      },
      "outputs": [],
      "source": [
        "# @title define rho0 (sampler) , rho1 (sampler) , real solution\n",
        "\n",
        "Sigma0 = torch.eye(dim)\n",
        "Sigma0[0, 0] = 1/4\n",
        "sqrt_Sigma0 = torch.sqrt(Sigma0)\n",
        "inv_sqrt_Sigma0 = torch.inverse(sqrt_Sigma0)\n",
        "mu0 = torch.zeros( 1, dim )\n",
        "\n",
        "def rho0(n, d=dim):\n",
        "    z = torch.randn(n, d)\n",
        "    Sigma0_z = torch.matmul(z, sqrt_Sigma0)\n",
        "    x = Sigma0_z + mu0\n",
        "\n",
        "    return x.cuda()\n",
        "\n",
        "\n",
        "Sigma1 = torch.eye(dim)\n",
        "Sigma1[1, 1] = 1/4\n",
        "Sigma1[3, 3] = 5/8\n",
        "Sigma1[4, 3] = 3/8\n",
        "Sigma1[3, 4] = 3/8\n",
        "Sigma1[4, 4] = 5/8\n",
        "sqrt_Sigma1 = torch.eye(dim)\n",
        "sqrt_Sigma1[1, 1] = 1/2\n",
        "sqrt_Sigma1[3, 3] = 3/4\n",
        "sqrt_Sigma1[4, 3] = 1/4\n",
        "sqrt_Sigma1[3, 4] = 1/4\n",
        "sqrt_Sigma1[4, 4] = 3/4\n",
        "inv_sqrt_Sigma1 = torch.eye(dim)\n",
        "inv_sqrt_Sigma1[1, 1] = 2\n",
        "inv_sqrt_Sigma1[3, 3] = 3/2\n",
        "inv_sqrt_Sigma1[4, 3] = -1/2\n",
        "inv_sqrt_Sigma1[3, 4] = -1/2\n",
        "inv_sqrt_Sigma1[4, 4] = 3/2\n",
        "mu1 = torch.zeros( 1, dim )\n",
        "def rho1(n, d=dim):\n",
        "    z = torch.randn(n, d)\n",
        "    Sigma1_z = torch.matmul(z, sqrt_Sigma1)\n",
        "    x = Sigma1_z + mu1\n",
        "\n",
        "    return x.cuda()\n",
        "\n",
        "\n",
        "# real sol: u(x) = 1/2 x^TAx + b^Tx\n",
        "# A = sqrt(Sigma_0^{-1}Sigma_1)  [Assume that Sigma_0, Sigma_1 are diagonal]\n",
        "# b = mu_1 - A mu_0\n",
        "A = torch.matmul(inv_sqrt_Sigma0, sqrt_Sigma1)\n",
        "b = mu1 - torch.matmul(mu0, A)\n",
        "A = A.cuda()\n",
        "b = b.cuda()\n",
        "# \\nabla u(x) = Ax+b\n",
        "def grad_u_real(x, d=dim):\n",
        "    gradients = torch.matmul(x, A) + b\n",
        "    return gradients.cuda()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "77uiwIHFhZzr"
      },
      "outputs": [],
      "source": [
        "# @title L2 error\n",
        "\n",
        "\n",
        "def gradient_nn(network, x):\n",
        "\n",
        "    input_variable = autograd.Variable(x, requires_grad=True)\n",
        "    output_value = network(input_variable)\n",
        "    gradients_x = autograd.grad(outputs=output_value, inputs=input_variable, grad_outputs=torch.ones(output_value.size()).cuda(), create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "\n",
        "    return gradients_x\n",
        "\n",
        "\n",
        "def L2_error(net_u, N):  # L2 norm of \\nabla net_u - \\nabla u_real\n",
        "\n",
        "    samples = rho0(N)\n",
        "    grad_realsolution = grad_u_real(samples)\n",
        "    grad_u_x = gradient_nn(net_u, samples)\n",
        "    L2_error = torch.sqrt(((grad_u_x - grad_realsolution)*(grad_u_x - grad_realsolution)).mean())\n",
        "\n",
        "    return L2_error\n",
        "\n",
        "\n",
        "def L2_error_T(net_T, N):  # L2 norm of \\nabla net_u - \\nabla u_real\n",
        "\n",
        "    samples = rho0(N)\n",
        "    grad_realsolution = grad_u_real(samples)\n",
        "    T_x = net_T(samples)\n",
        "    L2_error = torch.sqrt(((T_x - grad_realsolution)*(T_x - grad_realsolution)).mean())\n",
        "\n",
        "    return L2_error\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OqjDUweNxBtm"
      },
      "outputs": [],
      "source": [
        "# @title PDHG loss\n",
        "\n",
        "\n",
        "def gradient_nn(network, x):\n",
        "    input_variable = autograd.Variable(x, requires_grad=True)\n",
        "    output_value = network(input_variable)\n",
        "    gradients_x = autograd.grad(outputs=output_value, inputs=input_variable, grad_outputs=torch.ones(output_value.size()).cuda(), create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "    return gradients_x\n",
        "\n",
        "\n",
        "# PDHG loss with no boundary error\n",
        "def PDHG_loss1(net_u, net_phi, rho0_samples, rho1_samples):\n",
        "    grad_u_x = gradient_nn(net_u, rho0_samples)\n",
        "    loss = net_phi(grad_u_x) - net_phi(rho1_samples)\n",
        "    return loss.mean()\n",
        "\n",
        "\n",
        "# derived from the Monge problem\n",
        "def PDHG_loss2(net_u, net_phi, rho0_samples, rho1_samples):\n",
        "    grad_u_x = gradient_nn(net_u, rho0_samples)\n",
        "    loss = - torch.sum(rho0_samples * grad_u_x, -1).mean() + net_phi(grad_u_x).mean() - net_phi(rho1_samples).mean()\n",
        "    return loss\n",
        "\n",
        "\n",
        "def PDHG_loss3(net_T, net_phi, rho0_samples, rho1_samples):\n",
        "    T_x = net_T(rho0_samples)\n",
        "    loss = - torch.sum(rho0_samples * T_x, -1).mean() + net_phi(T_x).mean() - net_phi(rho1_samples).mean()\n",
        "    return loss\n",
        "\n",
        "\n",
        "def PDHG_loss4_with_extrapolation(net_T, net_phi, net_phi0, rho0_samples, rho1_samples, omega):\n",
        "    T_x = net_T(rho0_samples)\n",
        "    sqr_term = 1/2*torch.sum((T_x - rho0_samples)*(T_x - rho0_samples), -1).mean()\n",
        "    phi_T_x = net_phi(T_x)\n",
        "    phi0_T_x = net_phi0(T_x)\n",
        "    tilde_phi_T_x = phi_T_x +  omega * (phi_T_x - phi0_T_x)\n",
        "    phi_y = net_phi(rho1_samples)\n",
        "    phi0_y = net_phi0(rho1_samples)\n",
        "    tilde_phi_y = phi_y +  omega * (phi_y - phi0_y)\n",
        "    loss = sqr_term + tilde_phi_T_x.mean() - tilde_phi_y.mean()\n",
        "    return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVbj7yYdsqVz",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title G(\\theta) as a linear opt another\n",
        "import scipy\n",
        "from scipy.sparse.linalg import LinearOperator\n",
        "import torch.autograd.functional as functional\n",
        "\n",
        "\n",
        "def tensor_to_numpy(u):\n",
        "    if u.device==\"cpu\":\n",
        "        return u.detach().numpy()\n",
        "    else:\n",
        "        return u.cpu().detach().numpy()\n",
        "\n",
        "\n",
        "# explicitly form the Gram metric G(\\theta). Only for verification.\n",
        "def form_metric_tensor(input_dim, net, G_samples, device):\n",
        "\n",
        "    N = G_samples.size()[0]\n",
        "\n",
        "    Jacobi_NN = jacrev(functional_call, argnums = 1)\n",
        "    D_param_D_x_NN = vmap(jacrev(Jacobi_NN, argnums = 2), in_dims = (None, None, 0))\n",
        "    D_param_D_x_net_on_x = D_param_D_x_NN(net, dict(net.named_parameters()), G_samples)\n",
        "    num_params = torch.nn.utils.parameters_to_vector(net.parameters()).size()[0]\n",
        "    list_of_vectorized_param_gradients = []\n",
        "    for param_gradients in dict(D_param_D_x_net_on_x).items():\n",
        "        vectorized_param_gradients = param_gradients[1].view(N, -1, input_dim)\n",
        "        list_of_vectorized_param_gradients.append(vectorized_param_gradients)\n",
        "    total_vectorized_param_gradients = torch.cat(list_of_vectorized_param_gradients, 1)\n",
        "    transpose_total_vectorized_param_gradients = torch.transpose(total_vectorized_param_gradients, 1, 2)\n",
        "    batched_metric_tensor = torch.matmul(total_vectorized_param_gradients, transpose_total_vectorized_param_gradients)\n",
        "    metric_tensor = torch.mean(batched_metric_tensor, 0)\n",
        "\n",
        "    return metric_tensor\n",
        "\n",
        "\n",
        "# pull back Lap operator (as metric tensor)\n",
        "def metric_tensor_as_nabla_op(net, net_auxil, G_samples, vec, device):\n",
        "\n",
        "    num_params = len(torch.nn.utils.parameters_to_vector(net.parameters()))\n",
        "\n",
        "    params_net = dict(net.named_parameters())\n",
        "    params_net_auxil = dict(net_auxil.named_parameters())\n",
        "    ################### computation starts here ##################################\n",
        "    net.zero_grad()\n",
        "    net_auxil.zero_grad()\n",
        "\n",
        "    grad_net = vmap(jacrev(functional_call, argnums=2), in_dims=(None, None, 0))\n",
        "    grad_net_x = grad_net(net, params_net, G_samples)\n",
        "    grad_net_auxil_x =grad_net(net_auxil, params_net_auxil, G_samples)\n",
        "    ave_sqr_grad_net = torch.sum(grad_net_x * grad_net_auxil_x) / G_samples.size()[0]  # torch.sum(grad_net_x * grad_net_auxil_x, 2).mean()\n",
        "\n",
        "    nabla_theta_ave_sqr_grad_net = torch.autograd.grad(ave_sqr_grad_net, net_auxil.parameters(), grad_outputs=None ,allow_unused=True, retain_graph=True, create_graph=True)\n",
        "    vectorize_nabla_theta_ave_sqr_grad_net = torch.nn.utils.parameters_to_vector(nabla_theta_ave_sqr_grad_net)\n",
        "    vec_dot_nabla_theta_ave_sqr_grad_net = vectorize_nabla_theta_ave_sqr_grad_net.dot(vec)\n",
        "    metric_tensor_mult_vec = torch.autograd.grad(vec_dot_nabla_theta_ave_sqr_grad_net, net.parameters(), grad_outputs=None,allow_unused=True, retain_graph=True, create_graph=True)\n",
        "    vectorize_metric_tensor_mult_vec = torch.nn.utils.parameters_to_vector(metric_tensor_mult_vec)\n",
        "\n",
        "    return vectorize_metric_tensor_mult_vec\n",
        "\n",
        "\n",
        "# pull back identity operator (as metric tensor)\n",
        "def metric_tensor_as_op_identity_part(net, net_auxil, G_samples, vec, device):\n",
        "\n",
        "    num_params = len(torch.nn.utils.parameters_to_vector(net.parameters()))\n",
        "\n",
        "    params_net = dict(net.named_parameters())\n",
        "    params_net_auxil = dict(net_auxil.named_parameters())\n",
        "    ################### computation starts here ##################################\n",
        "    net_x = net(G_samples)\n",
        "    net_auxil_x = net_auxil(G_samples)\n",
        "    ave_net = torch.sum(net_x * net_auxil_x) / G_samples.size()[0]\n",
        "    nabla_theta_ave_net = torch.autograd.grad(ave_net, net_auxil.parameters(), grad_outputs=None ,allow_unused=True, retain_graph=True, create_graph=True)\n",
        "    vectorize_nabla_theta_net = torch.nn.utils.parameters_to_vector(nabla_theta_ave_net)\n",
        "    vec_dot_nabla_theta_ave_net = vectorize_nabla_theta_net.dot(vec)\n",
        "    metric_tensor_mult_vec = torch.autograd.grad(vec_dot_nabla_theta_ave_net, net.parameters(), grad_outputs=None,allow_unused=True, retain_graph=True, create_graph=True)\n",
        "    vectorize_metric_tensor_mult_vec = torch.nn.utils.parameters_to_vector(metric_tensor_mult_vec)\n",
        "\n",
        "    return vectorize_metric_tensor_mult_vec\n",
        "\n",
        "\n",
        "# G1: \\mathcal M = Id operator\n",
        "# G2: \\mathcal M = â–½ operator\n",
        "def minres_solver_G(net, net_auxil, interior_samples, boundary_samples, RHS_vec, device, bd_lambda, max_iternum, minres_tolerance, G_type):\n",
        "\n",
        "    num_params = torch.nn.utils.parameters_to_vector(net.parameters()).size()[0]\n",
        "\n",
        "    def G1_as_operator(vec):  # input the vector v [on CPU], return vector Gv\n",
        "        tensorized_vec = torch.Tensor(vec).to(device)\n",
        "        Gv = metric_tensor_as_op_identity_part(net, net_auxil, interior_samples, tensorized_vec, device)\n",
        "        return tensor_to_numpy(Gv)\n",
        "\n",
        "    def G2_as_operator(vec):  # input the vector v [on CPU], return vector Gv\n",
        "        tensorized_vec = torch.Tensor(vec).to(device)\n",
        "        Gv = metric_tensor_as_nabla_op(net, net_auxil, interior_samples, tensorized_vec, device)\n",
        "        return tensor_to_numpy(Gv)\n",
        "\n",
        "    if G_type == \"1\":\n",
        "        G_operator = LinearOperator((num_params, num_params), matvec=G1_as_operator)\n",
        "    elif G_type == \"2\":\n",
        "        G_operator = LinearOperator((num_params, num_params), matvec=G2_as_operator)\n",
        "    else:\n",
        "        print(\"Wrong G_type\")\n",
        "\n",
        "    np_RHS_vec = tensor_to_numpy(RHS_vec)\n",
        "    sol_vec, info = scipy.sparse.linalg.minres(G_operator, np_RHS_vec, rtol=minres_tolerance)\n",
        "    if (torch.max(torch.isnan(torch.tensor(sol_vec))) > 0):\n",
        "        print(\"Got the NAN!!!\")\n",
        "        sol_vec = np_RHS_vec\n",
        "        info = 0\n",
        "    tensorized_sol_vec = torch.Tensor(sol_vec).to(device)\n",
        "    return tensorized_sol_vec, info #, norm_err_vec\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5kyfJ8Mpd7w7"
      },
      "outputs": [],
      "source": [
        "# @title update param via line search\n",
        "\n",
        "def update_param(number_stepsizes, stepsize_0, base_stepsize, theta_0, tangent_theta, net_T_or_u, hidden_dim_net_T_or_u, net_phi, hidden_dim_net_phi, rho0_samples, rho1_samples, lossfunction, net_type, descent_or_ascent):\n",
        "\n",
        "    stepsize_list = stepsize_0 * base_stepsize**np.arange(number_stepsizes)\n",
        "    min_index = 0\n",
        "    index = 0\n",
        "    loss_along_stepsizes = []\n",
        "    current_min = 1000\n",
        "\n",
        "    if descent_or_ascent == \"descent\":\n",
        "        flag = -1\n",
        "    else:\n",
        "        flag = 1\n",
        "\n",
        "    if net_type == \"T\":\n",
        "        net_test = network_prim(network_length, dim, hidden_dim_net_T_or_u, dim).to(device)\n",
        "    elif net_type == \"u\":\n",
        "        net_test = network_prim(network_length, dim, hidden_dim_net_T_or_u, 1).to(device)\n",
        "    elif net_type == \"phi\":\n",
        "        net_test = network_dual(network_length, dim, hidden_dim_net_phi, 1).to(device)\n",
        "    else:\n",
        "        print(\"Wrong net_type\")\n",
        "\n",
        "    for stepsize in stepsize_list:\n",
        "        updated_theta = theta_0 + flag * stepsize * tangent_theta\n",
        "        torch.nn.utils.vector_to_parameters(updated_theta, net_test.parameters())\n",
        "        if net_type == \"T\" or net_type == \"u\":\n",
        "            loss = lossfunction(net_test, net_phi, rho0_samples, rho1_samples)\n",
        "        elif net_type == \"phi\":\n",
        "            loss = lossfunction(net_T_or_u, net_test, rho0_samples, rho1_samples)\n",
        "        else:\n",
        "            print(\"Wrong net_type\")\n",
        "        loss_along_stepsizes.append( - flag * loss.cpu().detach().numpy() )\n",
        "        if loss < current_min:\n",
        "            min_index = index\n",
        "            current_min = loss\n",
        "\n",
        "        index = index + 1\n",
        "\n",
        "    optimal_stepsize = stepsize_list[min_index]\n",
        "    optimal_updated_theta = theta_0 + flag * optimal_stepsize * tangent_theta\n",
        "\n",
        "    return optimal_updated_theta, optimal_stepsize, loss_along_stepsizes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "j_eB-c838Ulz"
      },
      "outputs": [],
      "source": [
        "# @title PD Adam solver\n",
        "\n",
        "\n",
        "def PD_Adam_solver(device, save_path, L, N,\n",
        "                          network_length, hidden_dimension_net_T, hidden_dimension_net_phi, flag_init,\n",
        "                          lr_T, lr_phi, iter, phi_iter, T_iter,\n",
        "                          plot_period, print_period, N_plot, chosen_dim_0, chosen_dim_1, chosen_dim_2, chosen_dim_3):\n",
        "\n",
        "    torch.manual_seed(50)\n",
        "\n",
        "    # initialize nets\n",
        "    net_T = network_prim(network_length, dim, hidden_dimension_net_T, dim).to(device)\n",
        "    optim_T = torch.optim.Adam(net_T.parameters(), lr=lr_T)\n",
        "    net_phi = network_dual(network_length, dim, hidden_dimension_net_phi, 1).to(device)\n",
        "    optim_phi = torch.optim.Adam(net_phi.parameters(), lr=lr_phi)\n",
        "    if flag_init == True:\n",
        "        net_T.initialization()\n",
        "        net_phi.initialization()\n",
        "\n",
        "    # initial err\n",
        "    error_0 = L2_error_T(net_T, 1800)\n",
        "    print(\"initial error = {}\".format(error_0.cpu().detach().numpy()))\n",
        "\n",
        "    loss = PDHG_loss3\n",
        "\n",
        "    rho0_samples = rho0(N)\n",
        "    plot_T_with_grad_ureal(0, rho0_samples, L, chosen_dim_0, chosen_dim_1, net_T, save_path)\n",
        "    plot_pushfwded_samples(0, net_T, rho0_samples, chosen_dim_0, chosen_dim_1, save_path=save_path)\n",
        "    plot_T_with_grad_ureal(0, rho0_samples, L, chosen_dim_2, chosen_dim_3, net_T, save_path)\n",
        "    plot_pushfwded_samples(0, net_T, rho0_samples, chosen_dim_2, chosen_dim_3, save_path=save_path)\n",
        "\n",
        "    l2error_list = []\n",
        "    ######################################################### PDHG iterations START HERE ###################################################################################################################\n",
        "    for t in range(iter):\n",
        "\n",
        "        rho0_samples = rho0(N)\n",
        "        rho1_samples = rho1(N)\n",
        "        ############################# update phi_\\eta #####################################\n",
        "        for inner_iter in range(phi_iter):\n",
        "            optim_phi.zero_grad()\n",
        "            lossa = - loss(net_T, net_phi,  rho0_samples, rho1_samples)\n",
        "            lossa.backward(retain_graph=True)\n",
        "            optim_phi.step()\n",
        "\n",
        "        ######################## update theta ##################################\n",
        "        for inner_iter in range(T_iter):\n",
        "            optim_T.zero_grad()\n",
        "            lossb = loss(net_T, net_phi, rho0_samples, rho1_samples)\n",
        "            lossb.backward(retain_graph=True)\n",
        "            optim_T.step()\n",
        "\n",
        "        ################ plot ##################\n",
        "        if (t+1) % plot_period == 0:\n",
        "            plot_T_with_grad_ureal(t+1, rho0_samples, L, chosen_dim_0, chosen_dim_1, net_T, save_path)\n",
        "            plot_pushfwded_samples(t+1, net_T, rho0_samples, chosen_dim_0, chosen_dim_1, save_path=save_path)\n",
        "            plot_T_with_grad_ureal(t+1, rho0_samples, L, chosen_dim_2, chosen_dim_3, net_T, save_path)\n",
        "            plot_pushfwded_samples(t+1, net_T, rho0_samples, chosen_dim_2, chosen_dim_3, save_path=save_path)\n",
        "\n",
        "        ##############\n",
        "        if (t+1) % print_period == 0:\n",
        "            print(\"Iteration: {}, \".format(t))\n",
        "            L2error = L2_error_T(net_T, 1800)\n",
        "            l2error_list.append(L2error.cpu().detach())\n",
        "            print(\"L2 error = {}\".format(L2error))\n",
        "\n",
        "\n",
        "    ######################################################### PDHG iterations END HERE #################################################################################################################\n",
        "    # save the models\n",
        "    save_path = os.getcwd()\n",
        "    filename = os.path.join(save_path, 'netT_PD_Adam.pt')\n",
        "    torch.save(net_T.state_dict(), filename)\n",
        "\n",
        "    save_path = os.getcwd()\n",
        "    filename = os.path.join(save_path, 'netphi_PD_Adam.pt')\n",
        "    torch.save(net_phi.state_dict(), filename)\n",
        "\n",
        "    # write down the error\n",
        "    with open('l2error_list', 'wb') as file1:\n",
        "        pickle.dump(l2error_list, file1)\n",
        "\n",
        "    # plot the error decay\n",
        "    fig_plot = plt.figure(figsize=(20, 20))\n",
        "    plt.plot(range(0, len(l2error_list)), log(l2error_list)/log(10))\n",
        "    plt.title(\"plot of log_10(L2 error)\")\n",
        "    fig_plot.savefig(\"Plot of the log l2 error\"+'.pdf')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "I1UBAe2qPXQ_"
      },
      "outputs": [],
      "source": [
        "# @title apply Primal-Dual Adam solver\n",
        "save_path = os.getcwd()\n",
        "\n",
        "\n",
        "# In Google Colab, change to working directory\n",
        "os.chdir(\"/content/drive/MyDrive/NPDG_codes_collection/OT_Gaussian\")\n",
        "# Verify\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "\n",
        "# create folder for NPDHG\n",
        "os.makedirs('PD Adam', exist_ok=True)\n",
        "os.chdir('PD Adam')\n",
        "\n",
        "L = 3.0\n",
        "N = 1000\n",
        "\n",
        "iter = 1 # 20000\n",
        "phi_iter = 1\n",
        "T_iter = 1\n",
        "lr_T = 0.5 * 1e-4\n",
        "lr_phi = 0.5 * 1e-4\n",
        "\n",
        "network_length = 4  #  2\n",
        "hidden_dimension_net_T = 80  #  50  # 100  # 600  # 40\n",
        "hidden_dimension_net_phi = 80  #  50  # 150  # 600  # 50\n",
        "flag_init = False\n",
        "\n",
        "plot_period = 1 # 1500\n",
        "print_period = 1 # 200\n",
        "N_plot = 100\n",
        "chosen_dim_0 = 0\n",
        "chosen_dim_1 = 1\n",
        "chosen_dim_2 = 3\n",
        "chosen_dim_3 = 4\n",
        "\n",
        "PD_Adam_solver( device, save_path, L, N,\n",
        "                network_length, hidden_dimension_net_T, hidden_dimension_net_phi, flag_init,\n",
        "                lr_T, lr_phi, iter, phi_iter, T_iter,\n",
        "                plot_period, print_period, N_plot, chosen_dim_0, chosen_dim_1, chosen_dim_2, chosen_dim_3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IlOU3RtI8Z8i"
      },
      "outputs": [],
      "source": [
        "# @title NPDHG solver\n",
        "import pickle\n",
        "\n",
        "\n",
        "def NPDHG_solver(device, save_path, L, N,\n",
        "                minres_max_iter, minres_tol,\n",
        "                network_length, hidden_dimension_net_T, hidden_dimension_net_phi, flag_init,\n",
        "                iter, phi_iter, T_iter, omega,\n",
        "                plot_period, print_period, N_plot, chosen_dim_0, chosen_dim_1, chosen_dim_2, chosen_dim_3,\n",
        "                number_stepsizes, base_stepsize,\n",
        "                precond_type,  # \"pullback_id_u_phi\", \"pullback_id_u_grad_phi\", \"pullback_grad_u_phi\"\n",
        "                stepsize_0=0.2,\n",
        "                adaptive_or_fixed_stepsize=\"fixed\", tau_T = 0.5 * 1e-3, tau_phi = 0.9 * 1e-3):\n",
        "\n",
        "    torch.manual_seed(50)\n",
        "\n",
        "    # initialize nets\n",
        "    net_T = network_prim(network_length, dim, hidden_dimension_net_T, dim).to(device)\n",
        "    net_phi = network_dual(network_length, dim, hidden_dimension_net_phi, 1).to(device)\n",
        "    if flag_init == True:\n",
        "        net_T.initialization()\n",
        "        net_phi.initialization()\n",
        "\n",
        "    # initial err\n",
        "    error_0 = L2_error_T(net_T, 2000)\n",
        "    print(error_0)\n",
        "\n",
        "    if precond_type == \"MpMd_id\":\n",
        "       G_T_type = \"1\"\n",
        "       G_phi_type = \"1\"\n",
        "    elif precond_type == \"Mp_id_Md_nabla\":\n",
        "       G_T_type = \"1\"\n",
        "       G_phi_type = \"2\"\n",
        "    else:\n",
        "       print(\"Wrong precond_type\")\n",
        "\n",
        "\n",
        "    loss = PDHG_loss3\n",
        "\n",
        "    rho0_samples = rho0(N)\n",
        "    plot_T_with_grad_ureal(0, rho0_samples, L, chosen_dim_0, chosen_dim_1, net_T, save_path)\n",
        "    plot_T_with_grad_ureal(0, rho0_samples, L, chosen_dim_2, chosen_dim_3, net_T, save_path)\n",
        "    plot_pushfwded_samples(0, net_T, rho0_samples, chosen_dim_0, chosen_dim_1, save_path=save_path)\n",
        "    plot_pushfwded_samples(0, net_T, rho0_samples, chosen_dim_2, chosen_dim_3, save_path=save_path)\n",
        "\n",
        "\n",
        "    comptime = []\n",
        "    total_time = 0\n",
        "    l2error_list = []\n",
        "    preconded_nabla_eta_norm_list = []\n",
        "    preconded_nabla_theta_norm_list = []\n",
        "    ######################################################### PDHG iterations START HERE ###################################################################################################################\n",
        "    for t in range(iter):\n",
        "\n",
        "        t_0 = time.time()\n",
        "\n",
        "        rho0_samples = rho0(N)\n",
        "        rho1_samples = rho1(N)\n",
        "        net_phi0 = network_dual(network_length, dim, hidden_dimension_net_phi, 1).to(device)\n",
        "        net_phi0.load_state_dict(net_phi.state_dict())\n",
        "\n",
        "        net_T.zero_grad()\n",
        "        net_phi.zero_grad()\n",
        "        ############################# update phi_\\eta #####################################\n",
        "        for inner_iter in range(phi_iter):\n",
        "            ############### compute G(\\eta)^{-1} \\nabla_\\eta loss() #########################\n",
        "            lossa = loss(net_T, net_phi,  rho0_samples, rho1_samples)\n",
        "            nabla_eta_loss = torch.autograd.grad(lossa, net_phi.parameters(), grad_outputs=None, allow_unused=True, retain_graph=True, create_graph=True)\n",
        "            vectorized_nabla_eta_loss = torch.nn.utils.parameters_to_vector(nabla_eta_loss)\n",
        "\n",
        "            # copy net_phi for G(\\eta) computation\n",
        "            net_phi_auxil = network_dual(network_length, dim, hidden_dimension_net_phi, 1).to(device)\n",
        "            net_phi_auxil.load_state_dict(net_phi.state_dict())\n",
        "\n",
        "            # compute G(\\eta)^{-1} \\nabla_\\eta loss()\n",
        "            G_inv_nabla_eta_loss, info_phi = minres_solver_G(net_phi, net_phi_auxil, rho1_samples, None, vectorized_nabla_eta_loss, device, None, minres_max_iter, minres_tol, G_phi_type)\n",
        "\n",
        "            # update \\eta\n",
        "            original_eta = torch.nn.utils.parameters_to_vector(net_phi.parameters())\n",
        "            if adaptive_or_fixed_stepsize == \"adaptive\":\n",
        "              updated_eta, tau_phi, value_along_tau_phis = update_param(number_stepsizes, stepsize_0, base_stepsize, original_eta, G_inv_nabla_eta_loss, net_T, hidden_dimension_net_T, net_phi, hidden_dimension_net_phi, rho0_samples, rho1_samples, loss, \"phi\", \"ascent\")\n",
        "            elif adaptive_or_fixed_stepsize == \"fixed\":\n",
        "               updated_eta = original_eta + tau_phi * G_inv_nabla_eta_loss\n",
        "            else:\n",
        "               raise ValueError(\"adaptive_or_fixed_stepsize must be 'adaptive' or 'fixed'\")\n",
        "            torch.nn.utils.vector_to_parameters(updated_eta, net_phi.parameters())\n",
        "\n",
        "        ######################## update theta ##################################\n",
        "        for inner_iter in range(T_iter):\n",
        "            # compute G(\\theta)^{-1} \\nabla_\\theta loss()\n",
        "            lossb = PDHG_loss4_with_extrapolation(net_T, net_phi, net_phi0, rho0_samples, rho1_samples, omega)\n",
        "            nabla_theta_loss = torch.autograd.grad(lossb, net_T.parameters(), grad_outputs=None, allow_unused=True, retain_graph=True, create_graph=True)\n",
        "            vectorized_nabla_theta_loss = torch.nn.utils.parameters_to_vector(nabla_theta_loss)\n",
        "\n",
        "            # copy net_T for  G(\\theta) computation\n",
        "            net_T_auxil = network_prim(network_length, dim, hidden_dimension_net_T, dim).to(device)\n",
        "            net_T_auxil.load_state_dict(net_T.state_dict())\n",
        "\n",
        "            # compute G(\\theta)^{-1} \\nabla_\\theta loss()\n",
        "            G_inv_nabla_theta_loss, info_T = minres_solver_G(net_T, net_T_auxil, rho0_samples, None, vectorized_nabla_theta_loss, device, None, minres_max_iter, minres_tol, G_T_type)\n",
        "\n",
        "            ############# update theta ####################\n",
        "            original_theta = torch.nn.utils.parameters_to_vector(net_T.parameters())\n",
        "            if adaptive_or_fixed_stepsize == \"adaptive\":\n",
        "               updated_theta, tau_T, value_along_tau_Ts = update_param(number_stepsizes, stepsize_0, base_stepsize, original_theta, G_inv_nabla_theta_loss, net_T, hidden_dimension_net_T, net_phi, hidden_dimension_net_phi, rho0_samples, rho1_samples, loss, \"T\", \"descent\")\n",
        "            elif adaptive_or_fixed_stepsize == \"fixed\":\n",
        "               updated_theta = original_theta - tau_T * G_inv_nabla_theta_loss\n",
        "            else:\n",
        "               raise ValueError(\"adaptive_or_fixed_stepsize must be 'adaptive' or 'fixed'\")\n",
        "            torch.nn.utils.vector_to_parameters(updated_theta, net_T.parameters())\n",
        "\n",
        "        t_1 = time.time()\n",
        "        total_time += t_1 - t_0\n",
        "        comptime.append(total_time)\n",
        "\n",
        "        ################ plot ##################\n",
        "        if (t+1) % plot_period == 0:\n",
        "            plot_T_with_grad_ureal(t+1, rho0_samples, L, chosen_dim_0, chosen_dim_1, net_T, save_path)\n",
        "            plot_pushfwded_samples(t+1, net_T, rho0_samples, chosen_dim_0, chosen_dim_1, save_path=save_path)\n",
        "            plot_T_with_grad_ureal(t+1, rho0_samples, L, chosen_dim_2, chosen_dim_3, net_T, save_path)\n",
        "            plot_pushfwded_samples(t+1, net_T, rho0_samples, chosen_dim_2, chosen_dim_3, save_path=save_path)\n",
        "\n",
        "        ##############\n",
        "        L2error = L2_error_T(net_T, 1800)\n",
        "        l2error_list.append(L2error.cpu().detach())\n",
        "        if (t+1) % print_period == 0:\n",
        "            print(\"Iteration: {}, \".format(t))\n",
        "            print(\"L2 error = {}\".format(L2error))\n",
        "\n",
        "\n",
        "    ######################################################### PDHG iterations END HERE #################################################################################################################\n",
        "    # save the models\n",
        "    save_path = os.getcwd()\n",
        "    filename = os.path.join(save_path, 'netT_NPDHG.pt')\n",
        "    torch.save(net_T.state_dict(), filename)\n",
        "\n",
        "    save_path = os.getcwd()\n",
        "    filename = os.path.join(save_path, 'netphi_NPDHG.pt')\n",
        "    torch.save(net_phi.state_dict(), filename)\n",
        "\n",
        "\n",
        "    # write down the error\n",
        "    with open('l2error_list', 'wb') as file1:\n",
        "        pickle.dump(l2error_list, file1)\n",
        "    with open('comptime', 'wb') as f:\n",
        "        pickle.dump(comptime, f)\n",
        "\n",
        "\n",
        "    # plot the error decay\n",
        "    fig_plot = plt.figure(figsize=(20, 20))\n",
        "    plt.plot(range(0, len(l2error_list)), log(l2error_list)/log(10))\n",
        "    plt.title(\"plot of log_10(L2 error)\")\n",
        "    fig_plot.savefig(\"Plot of the log l2 error\"+'.pdf')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    # plot the error decay\n",
        "    fig_plot = plt.figure(figsize=(20, 20))\n",
        "    plt.plot(comptime, log(l2error_list)/log(10))\n",
        "    plt.title(\"log_10(L2 error) vs. computation time (seconds)\\n\", fontsize=40)\n",
        "    plt.xlabel(\"computation time (seconds)\", fontsize=30)\n",
        "    plt.ylabel(\"log_10(L2 error)\", fontsize=30)\n",
        "    fig_plot.savefig(\"Plot of the log l2 error vs comp time \"+'.pdf')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-zSuJ2pW-7OH",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title apply NPDHG solver\n",
        "save_path = os.getcwd()\n",
        "\n",
        "\n",
        "# In Google Colab, change to working directory\n",
        "os.chdir(\"/content/drive/MyDrive/NPDG_codes_collection/OT_Gaussian\")\n",
        "# Verify\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "\n",
        "# create folder for NPDHG\n",
        "os.makedirs('NPDHG precondition', exist_ok=True)\n",
        "os.chdir('NPDHG precondition')\n",
        "\n",
        "\n",
        "L = 3\n",
        "N = 2000\n",
        "\n",
        "minres_max_iter = 1000\n",
        "minres_tol = 1e-3\n",
        "\n",
        "network_length = 4\n",
        "hidden_dimension_net_T = 80\n",
        "hidden_dimension_net_phi = 80\n",
        "flag_init = False\n",
        "\n",
        "iter =  12000\n",
        "phi_iter = 1\n",
        "T_iter = 1\n",
        "omega = 1.0\n",
        "\n",
        "plot_period =  2000\n",
        "print_period =  100\n",
        "N_plot = 1000\n",
        "chosen_dim_0 = 0\n",
        "chosen_dim_1 = 1\n",
        "chosen_dim_2 = 3\n",
        "chosen_dim_3 = 4\n",
        "\n",
        "number_stepsizes = 50\n",
        "base_stepsize = 0.8\n",
        "\n",
        "# precond_type = \"MpMd_id\"\n",
        "precond_type = \"Mp_id_Md_nabla\"\n",
        "\n",
        "NPDHG_solver(device, save_path, L, N,\n",
        "            minres_max_iter, minres_tol,\n",
        "            network_length, hidden_dimension_net_T, hidden_dimension_net_phi, flag_init,\n",
        "            iter, phi_iter, T_iter, omega,\n",
        "            plot_period, print_period, N_plot, chosen_dim_0, chosen_dim_1, chosen_dim_2, chosen_dim_3,\n",
        "            number_stepsizes, base_stepsize,\n",
        "            precond_type,\n",
        "            adaptive_or_fixed_stepsize=\"fixed\", tau_T = 0.5 * 1e-1, tau_phi = 0.9 * 1e-1\n",
        "            )\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}