{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSYM5-A-QTlm",
        "outputId": "bf34ca87-86b1-4e02-edf8-ca1c96fdc1d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FHjjp6gI1nO",
        "outputId": "ebbdab53-d91c-4770-9087-a969f708be9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/NPDG_Github/RD\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/NPDG/RD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QSjLeuOZRg0t"
      },
      "outputs": [],
      "source": [
        "# @title import\n",
        "import math\n",
        "import random\n",
        "import scipy\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('agg')\n",
        "\n",
        "from matplotlib.pyplot import figure\n",
        "from mpl_toolkits import mplot3d\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from torch.func import grad, vmap\n",
        "from torch.func import jacrev\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from numpy import *\n",
        "from torch import Tensor\n",
        "from torch.nn import Parameter\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook computes the numerical solution to a 1D Allen-Cahn equation with $\\epsilon_0=0.01$ using the NPDG method.\n",
        "\n",
        "See Section 5.4 and Appendix F for more detailed description."
      ],
      "metadata": {
        "id": "KA5jxAe3E_P-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLjadoD5-hLM"
      },
      "source": [
        "\n",
        "\n",
        "Consider the reaction-diffusion (RD) equation (Allen-Cahn eq.) $$\\partial_t u = a\\Delta u - b f(u)$$ defined on $\\Omega = [0,2]$, $a=0.01$, $b=100$. Here $$f(u) = W'(u) = u^3 - u.$$\n",
        "Here $W(u)=\\frac14(u^2-1)^2.$\n",
        "\n",
        "Suppose we impose the Neumann boundary condition and initial condition $$u(x,0)=(1-\\cos(\\pi (x-1)))\\cos(\\pi (x-1)).$$\n",
        "\n",
        "<!-- Or\n",
        "$$u(x,0)=(1-\\cos(\\pi (x-1)))\\cos(\\pi (x-1))+0.5.$$ -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "feppnxyuX9ky"
      },
      "outputs": [],
      "source": [
        "# @title set dimension\n",
        "\n",
        "dim = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqmvN4WRcGEy",
        "outputId": "f07e80cb-8dfe-4038-d826-d31f89dd70ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# @title Check CUDA availability\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GV9LAaw3zM97"
      },
      "outputs": [],
      "source": [
        "# @title Sampler\n",
        "\n",
        "\n",
        "L = 2.0\n",
        "def rho_1_sampler(n, dim=dim):\n",
        "    samples = torch.arange(0, L, 0.001)\n",
        "    center_samples = torch.arange(L/2-0.2, L/2+0.2, 0.001)\n",
        "    totsample = torch.cat((samples, center_samples), 0)\n",
        "    totsample.unsqueeze_(1)\n",
        "    return totsample.cuda()\n",
        "\n",
        "\n",
        "def rho_bdry_sampler(n, dim=dim):\n",
        "    boundary_coord = np.random.randint(2, size=n) * L  # either 1 or 0\n",
        "    index_randint = np.random.randint(dim, size=n)\n",
        "    low_dim_sample = rho_1_sampler(n, dim-1).cpu()\n",
        "    samples = torch.zeros(n, dim)\n",
        "    for i in range(n):\n",
        "        x = np.array(low_dim_sample[i, :])\n",
        "        y = np.zeros(dim)\n",
        "        if index_randint[i] < dim-1:\n",
        "            y = np.insert(x, index_randint[i], boundary_coord[i])\n",
        "        else:\n",
        "            y = np.append(x, boundary_coord[i])\n",
        "        samples[i] = torch.tensor(y)\n",
        "    return samples.cuda()\n",
        "\n",
        "\n",
        "def rho_bdry_sampler_with_directional_vector(n, dim=dim, L=L):\n",
        "    if dim == 1:\n",
        "        samples = torch.zeros(2, 1)\n",
        "        samples[1] = L\n",
        "        outward_direction = torch.zeros(2, 1)\n",
        "        outward_direction[0] = -1\n",
        "        outward_direction[1] = 1\n",
        "    else:\n",
        "        boundary_coord = np.random.randint(2, size=n) * L  # either 1 or 0\n",
        "        index_randint = np.random.randint(dim, size=n)\n",
        "        low_dim_sample = rho_1_sampler(n, dim-1).cpu()\n",
        "        samples = torch.zeros(n, dim)\n",
        "        outward_direction = torch.zeros(n, dim)\n",
        "        for i in range(n):\n",
        "            x = np.array(low_dim_sample[i, :])\n",
        "            y = np.zeros(dim)\n",
        "            if index_randint[i] < dim-1:\n",
        "                y = np.insert(x, index_randint[i], boundary_coord[i])\n",
        "            else:\n",
        "                y = np.append(x, boundary_coord[i])\n",
        "            samples[i] = torch.tensor(y)\n",
        "            if boundary_coord[i] == L:\n",
        "                outward_direction[i, index_randint[i]] = 1\n",
        "            else:\n",
        "                outward_direction[i, index_randint[i]] = -1\n",
        "    return samples.cuda(), outward_direction.cuda()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4TtvnEDiNcIV"
      },
      "outputs": [],
      "source": [
        "# @title model (MLP, activation=Tanh)\n",
        "\n",
        "\n",
        "class network_prim(nn.Module):\n",
        "    def __init__(self, network_length, input_dimension, hidden_dimension, output_dimension):\n",
        "        super(network_prim, self).__init__()\n",
        "\n",
        "        self.network_length = network_length\n",
        "\n",
        "        self.linears = nn.ModuleList([nn.Linear(input_dimension, hidden_dimension)])\n",
        "        self.linears.extend([nn.Linear(hidden_dimension, hidden_dimension) for _ in range(1, network_length-1)])\n",
        "        self.linears.extend([nn.Linear(hidden_dimension, output_dimension, bias=False)])\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def initialization(self):\n",
        "        for l in self.linears:\n",
        "            l.weight.data.normal_()\n",
        "            if l.bias is not None:\n",
        "                l.bias.data.normal_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        l = self.linears[0]\n",
        "        x = l(x)\n",
        "        for l in self.linears[1: self.network_length-1]:\n",
        "            x = self.tanh(x)\n",
        "            x = l(x)\n",
        "        x = self.tanh(x)\n",
        "        l = self.linears[self.network_length-1]\n",
        "        x = l(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class network_dual(nn.Module):\n",
        "    def __init__(self, network_length, input_dimension, hidden_dimension, output_dimension, L):\n",
        "        super(network_dual, self).__init__()\n",
        "\n",
        "        self.network_length = network_length\n",
        "\n",
        "        self.linears = nn.ModuleList([nn.Linear(input_dimension, hidden_dimension)])\n",
        "        self.linears.extend([nn.Linear(hidden_dimension, hidden_dimension) for _ in range(1, network_length-1)])\n",
        "        self.linears.extend([nn.Linear(hidden_dimension, output_dimension)])\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.L = L\n",
        "\n",
        "    def initialization(self):\n",
        "        for l in self.linears:\n",
        "            l.weight.data.normal_()\n",
        "            if l.bias is not None:\n",
        "                l.bias.data.normal_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # modify = self.modif_function(x)\n",
        "        l = self.linears[0]\n",
        "        x = l(x)\n",
        "        for l in self.linears[1: self.network_length-1]:\n",
        "            x = self.tanh(x)\n",
        "            x = l(x)\n",
        "        x = self.tanh(x)\n",
        "        l = self.linears[self.network_length-1]\n",
        "        x = l(x)\n",
        "        # x = x * modify\n",
        "        return x\n",
        "\n",
        "\n",
        "class network_dual_on_bdry(nn.Module):\n",
        "    def __init__(self, network_length, input_dimension, hidden_dimension, output_dimension):\n",
        "        super(network_dual_on_bdry, self).__init__()\n",
        "\n",
        "        self.network_length = network_length\n",
        "\n",
        "        self.linears = nn.ModuleList([nn.Linear(input_dimension, hidden_dimension)])\n",
        "        self.linears.extend([nn.Linear(hidden_dimension, hidden_dimension) for _ in range(1, network_length-1)])\n",
        "        self.linears.extend([nn.Linear(hidden_dimension, output_dimension, bias=False)])\n",
        "\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "\n",
        "    def initialization(self):\n",
        "        for l in self.linears:\n",
        "            l.weight.data.normal_()\n",
        "            if l.bias is not None:\n",
        "                l.bias.data.normal_()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        l = self.linears[0]\n",
        "\n",
        "        x = l(x)\n",
        "        for l in self.linears[1: self.network_length-1]:\n",
        "            x = self.tanh(x)\n",
        "            x = l(x)\n",
        "        x = self.tanh(x)\n",
        "        l = self.linears[self.network_length-1]\n",
        "        x = l(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nzw6Dq1EMiB6"
      },
      "outputs": [],
      "source": [
        "# @title Plotting function\n",
        "\n",
        "\n",
        "def Plot_graph_nn_primal(t, net_primal, L, num_of_intervals, Iter, flag_plot_real, u_real, save_path, z_min, z_max, device, chosen_dim_0, chosen_dim_1, d=dim):\n",
        "    num_of_intervals = 100\n",
        "    h = L / num_of_intervals\n",
        "\n",
        "    coord = np.arange(num_of_intervals+1) * h\n",
        "    nodes = torch.zeros(num_of_intervals+1, 1)\n",
        "    nodes[:, 0] = torch.tensor(coord)\n",
        "    nodes = nodes.cuda()\n",
        "    u_nodes = net_primal(nodes)\n",
        "    u_0_nodes = u_0(nodes)\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 20))\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.set_xlim([0.0, L])\n",
        "    ax.set_ylim([z_min, z_max])\n",
        "    ax.plot(nodes.cpu().detach().numpy(), u_nodes.cpu().detach().numpy(), 'b-', linewidth = 2, label='u(t)')\n",
        "    ax.plot(nodes.cpu().detach().numpy(), u_0_nodes.cpu().detach().numpy(), 'g--', linewidth = 2, label='u(0)')\n",
        "    if flag_plot_real == 1:\n",
        "        ax.plot(nodes.cpu().detach().numpy(), u_real.cpu(), 'r--', linewidth = 2, label='u_IMEX(t)')\n",
        "    plt.xlabel(\"x-axis\")\n",
        "    plt.ylabel(\"y-axis\")\n",
        "    plt.legend(fontsize=50, loc=\"upper right\")\n",
        "    ax.set_title('Graph of u(t) at physical time t={} (Iteration = {})'.format(t, Iter), fontsize = 20)\n",
        "    filename = os.path.join(save_path, \"({}th Iteration) Graph of u_theta at physical time t={}\".format(Iter, t)+ '.pdf')\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def Plot_graph_nn_dual(t, net_dual, L, num_of_intervals, Iter, save_path, z_min, z_max, device, chosen_dim_0, chosen_dim_1, d=dim):\n",
        "    num_of_intervals = 100\n",
        "    h = L / num_of_intervals\n",
        "\n",
        "    coord = np.arange(num_of_intervals+1) * h\n",
        "    nodes = torch.zeros(num_of_intervals+1, 1)\n",
        "    nodes[:, 0] = torch.tensor(coord)\n",
        "    nodes = nodes.cuda()\n",
        "    u_nodes = net_dual(nodes)\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 20))\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.set_xlim([0.0, L])\n",
        "    ax.set_ylim([z_min, z_max])\n",
        "    ax.plot(nodes.cpu().detach().numpy(), u_nodes.cpu().detach().numpy(), 'b-', linewidth = 2, label='phi(t)')\n",
        "    plt.xlabel(\"x-axis\")\n",
        "    plt.ylabel(\"y-axis\")\n",
        "    plt.legend(fontsize=50, loc=\"upper right\")\n",
        "    ax.set_title('Graph of dual network phi at physical time t={} on {}-{} plane. (Iteration = {})'.format(t, chosen_dim_0, chosen_dim_1, Iter), fontsize = 20)\n",
        "    filename = os.path.join(save_path, \"({}th Iteration) Graph of phi_eta at physical time t={}\".format(Iter, t)+ '.pdf')\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def Plot_numerical_solution(t, u, L, num_of_intervals, save_path, z_min, z_max, device, chosen_dim_0, chosen_dim_1, d=dim):\n",
        "    num_of_intervals = 100\n",
        "    h = L / num_of_intervals\n",
        "\n",
        "    coord = np.arange(num_of_intervals+1) * h\n",
        "    nodes = torch.zeros(num_of_intervals+1, 1)\n",
        "    nodes[:, 0] = torch.tensor(coord)\n",
        "    nodes = nodes.cuda()\n",
        "    u_nodes = u\n",
        "    u_0_nodes = u_0(nodes)\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 20))\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.set_xlim([0.0, L])\n",
        "    ax.set_ylim([z_min, z_max])\n",
        "    ax.plot(nodes.cpu().detach().numpy(), u_nodes.cpu().detach().numpy(), 'b-', linewidth = 2, label='u(t) discrete numerical scheme')\n",
        "    ax.plot(nodes.cpu().detach().numpy(), u_0_nodes.cpu().detach().numpy(), 'g--', linewidth = 2, label='u(0)')\n",
        "    plt.xlabel(\"x-axis\")\n",
        "    plt.ylabel(\"y-axis\")\n",
        "    plt.legend(fontsize=50, loc=\"upper right\")\n",
        "    ax.set_title('Graph of implicit numerical scheme solution u(t) at physical time t={}'.format(t), fontsize = 20)\n",
        "    filename = os.path.join(save_path, \"  Graph of implicit numerical scheme solution u_theta at physical time t={}\".format( t)+ '.pdf')\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ukl93MWUML1H"
      },
      "outputs": [],
      "source": [
        "# @title Initial value, necessary constants\n",
        "\n",
        "def u_0(x):\n",
        "    u_value = (1 - torch.cos(math.pi * (x - 1))) * torch.cos(math.pi * (x - 1))\n",
        "    return u_value\n",
        "\n",
        "a = 0.01  # in this code, epsilon_0 is denoted as a\n",
        "b = 100.0\n",
        "def W(x):\n",
        "    return 1/4 * (x*x - 1)**2\n",
        "\n",
        "def dW(x):\n",
        "    return x*x*x - x\n",
        "\n",
        "def ddW(x):\n",
        "    return 3 * x*x - 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "c0P4qY4XniOF"
      },
      "outputs": [],
      "source": [
        "# @title Initial loss\n",
        "def Initial_loss(net_u, N):\n",
        "\n",
        "    samples = rho_1_sampler(N, dim)\n",
        "    realsolution = u_0(samples)\n",
        "    u_x = net_u(samples).cuda()\n",
        "    diff = realsolution - u_x\n",
        "    initial_loss = torch.sqrt((diff * diff).mean())\n",
        "\n",
        "    return initial_loss.cuda()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "77uiwIHFhZzr"
      },
      "outputs": [],
      "source": [
        "# @title L2 error\n",
        "\n",
        "def L2_error(net_u, ureal, N_x, L=L):\n",
        "    h_x = L / N_x\n",
        "    samples = (torch.arange(N_x + 1) * h_x).unsqueeze(-1).cuda()\n",
        "    ux = net_u(samples)\n",
        "    diff = ux - ureal.unsqueeze(-1)\n",
        "    error = torch.sqrt((diff*diff).mean())\n",
        "    return error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "W0pgggu3OEAK"
      },
      "outputs": [],
      "source": [
        "# @title fixed point solver used in solving the RD equation (1D)\n",
        "\n",
        "ave_ddW_bar_u = 2.0\n",
        "# solve the numerical solution on [0, t] with time stepsize h_t and space discretization h_x = L/N_x using the implicit scheme\n",
        "# every t_i solve\n",
        "# (I - ah_tΔ_hx) u + bh_tW'(u) = u_t_i-1\n",
        "# for u\n",
        "\n",
        "# fixed point\n",
        "# U_{k+1} = (I - ah_tΔ_hx)^{-1}(u_t_i-1 - bh_tW'(U_{k }))\n",
        "def Fixed_pt_solver_1D(t, h_t, L, N_x, iter_num_fixed_pt):\n",
        "\n",
        "    N_t = int(t / h_t)\n",
        "\n",
        "    h_x = L / N_x\n",
        "    k = h_t / (h_x * h_x)\n",
        "    nodes = torch.arange(N_x + 1) * h_x\n",
        "    u0 = u_0(nodes)\n",
        "\n",
        "    # matrix = I - a*k*Δ_hx\n",
        "    matrix_upperdiag = np.diag(np.ones(N_x), 1)\n",
        "    matrix_lowerdiag = np.diag(np.ones(N_x), -1)\n",
        "    Id = np.eye(N_x+1)\n",
        "    discrete_Lap = - 2 * Id + matrix_upperdiag + matrix_lowerdiag\n",
        "    discrete_Lap[0, 0] = -1\n",
        "    discrete_Lap[N_x, N_x] = -1\n",
        "    discrete_Lap_tensorized = torch.from_numpy(discrete_Lap)\n",
        "    matrix = (1 + b * h_t * ave_ddW_bar_u) * np.eye(N_x+1) - a * k * discrete_Lap\n",
        "    inv_matrix = np.linalg.inv(matrix)\n",
        "    inv_matrix = torch.from_numpy(inv_matrix)\n",
        "\n",
        "    ut = torch.zeros(N_t, N_x+1)\n",
        "    u_laststep = u0\n",
        "    for i in range(N_t):\n",
        "        # fixed pt\n",
        "        u_fixed_pt_k = u_laststep\n",
        "        for k in range(iter_num_fixed_pt):\n",
        "            rhs = u_laststep - b * h_t * (dW(u_fixed_pt_k) - ave_ddW_bar_u * u_fixed_pt_k)\n",
        "            u_fixed_pt_k = torch.matmul(inv_matrix, rhs.double())\n",
        "            # compute residue\n",
        "            res = (u_fixed_pt_k - u_laststep) / h_t - a * torch.matmul(discrete_Lap_tensorized, u_fixed_pt_k) / (h_x * h_x) + b * dW(u_fixed_pt_k)\n",
        "            print(torch.norm(res))\n",
        "            if torch.norm(res) < 1e-6:\n",
        "                break\n",
        "        u_laststep = u_fixed_pt_k\n",
        "        ut[i, :] = u_fixed_pt_k\n",
        "    return ut.cuda()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4UrzpsMcYXzO"
      },
      "outputs": [],
      "source": [
        "# @title IMEX solver for solving the reaction-diffusion equation (1D)\n",
        "\n",
        "# solve the numerical solution on [0, t] with time stepsize h_t and space discretization h_x = L/N_x using the IMEX scheme\n",
        "# every t_i solve\n",
        "# (I - ah_tΔ_hx) u = u_t_i-1 - bh_tf(u_t_i-1)\n",
        "# for u\n",
        "def IMEX_solver_1D(t, h_t, L, N_x):\n",
        "\n",
        "    N_t = int(t / h_t)\n",
        "\n",
        "    h_x = L / N_x\n",
        "    k = h_t / (h_x * h_x)\n",
        "    nodes = torch.arange(N_x + 1) * h_x\n",
        "    u0 = u_0(nodes)\n",
        "\n",
        "    # matrix = I - a*k*Δ_hx\n",
        "    matrix_upperdiag = np.diag(np.ones(N_x), 1)\n",
        "    matrix_lowerdiag = np.diag(np.ones(N_x), -1)\n",
        "    Id = np.eye(N_x+1)\n",
        "    discrete_Lap = - 2 * Id + matrix_upperdiag + matrix_lowerdiag\n",
        "    discrete_Lap[0, 0] = -1\n",
        "    discrete_Lap[N_x, N_x] = -1\n",
        "    matrix = np.eye(N_x+1) - a * k * discrete_Lap\n",
        "    inv_matrix = np.linalg.inv(matrix)\n",
        "    inv_matrix = torch.from_numpy(inv_matrix)\n",
        "\n",
        "    ut = torch.zeros(N_t, N_x+1)\n",
        "    u_laststep = u0\n",
        "    for i in range(N_t):\n",
        "        rhs = u_laststep - b * h_t * dW(u_laststep)\n",
        "        u_current = torch.matmul(inv_matrix, rhs.double())\n",
        "        ut[i, :] = u_current\n",
        "        u_laststep = u_current\n",
        "\n",
        "    return ut.cuda()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "M0upnUqnxPKt"
      },
      "outputs": [],
      "source": [
        "# @title computing Laplacian\n",
        "from torch.func import hessian, vmap, functional_call\n",
        "\n",
        "\n",
        "def v_compute_Laplacian(net, samples):\n",
        "    def compute_Laplacian(x):\n",
        "        hessian_net = hessian(net, argnums=0)(x) #forward-over-reverse hessian calc.\n",
        "        laplacian_net = hessian_net.diagonal(0,-2,-1) #use relative dims for vmap (function doesn't see the batch dim of the input)\n",
        "        return torch.sum(laplacian_net, -1)\n",
        "    Laplacian_wrt_x = vmap(compute_Laplacian)(samples)\n",
        "    return Laplacian_wrt_x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OqjDUweNxBtm"
      },
      "outputs": [],
      "source": [
        "# @title PDHG loss\n",
        "\n",
        "\n",
        "def gradient_nn(network, x):\n",
        "    input_variable = autograd.Variable(x, requires_grad=True)\n",
        "    output_value = network(input_variable)\n",
        "    gradients_x = autograd.grad(outputs=output_value, inputs=input_variable, grad_outputs=torch.ones(output_value.size()).cuda(), create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "    return gradients_x\n",
        "\n",
        "\n",
        "# PDHG loss with no boundary error\n",
        "# u_laststep = u_t-1(in_samples)\n",
        "def PDHG_loss_without_bd(net_u, u_laststep, net_phi, in_samples, h_t, ave_value_ddW, a=a, b=b):\n",
        "\n",
        "    u_x = net_u(in_samples)\n",
        "    phi_x = net_phi(in_samples)\n",
        "    grad_u = gradient_nn(net_u, in_samples)\n",
        "    grad_phi = gradient_nn(net_phi, in_samples)\n",
        "    dW_u = dW(u_x)\n",
        "    loss1 = ((u_x + b * h_t * dW_u - u_laststep)/h_t * phi_x).mean()\n",
        "    loss2 = a * (torch.sum(grad_u * grad_phi, -1).unsqueeze(-1)).mean()\n",
        "\n",
        "    return loss1 + loss2\n",
        "\n",
        "\n",
        "# PDHG loss\n",
        "def PDHG_loss(net_u, u_laststep, net_phi, net_psi, in_samples, bd_samples, h_t, ave_value_ddW, bd_lambda, a=a, b=b):\n",
        "\n",
        "    u_x = net_u(in_samples)\n",
        "    phi_x = net_phi(in_samples)\n",
        "    grad_u = gradient_nn(net_u, in_samples)\n",
        "    grad_phi = gradient_nn(net_phi, in_samples)\n",
        "    dW_u = dW(u_x)\n",
        "    loss1 = ((u_x + b * h_t * dW_u - u_laststep) * phi_x).mean()\n",
        "    loss2 = a * h_t * (torch.sum(grad_u * grad_phi, -1).unsqueeze(-1)).mean()\n",
        "    in_loss = loss1 + loss2\n",
        "\n",
        "    bdry_sample, outward_direction = rho_bdry_sampler_with_directional_vector(2)\n",
        "    psi_x = net_psi(bdry_sample)\n",
        "    grad_u = gradient_nn(net_u, bdry_sample)\n",
        "    directional_grad_u = torch.sum(outward_direction * grad_u, -1).unsqueeze(-1)\n",
        "    bd_loss = (directional_grad_u * psi_x).mean()\n",
        "\n",
        "    return in_loss + bd_lambda * bd_loss\n",
        "\n",
        "\n",
        "def PDHG_loss_typePINN(net_u, u_laststep, net_phi, net_psi, in_samples, bd_samples, h_t, ave_value_ddW, bd_lambda, a=a, b=b):\n",
        "    u_x = net_u(in_samples)\n",
        "    phi_x = net_phi(in_samples)\n",
        "    Lap_u = v_compute_Laplacian(net_u, in_samples)\n",
        "    dW_u = dW(u_x)\n",
        "    in_loss = ((u_x + h_t * (- a * Lap_u + b * dW_u) - u_laststep) * phi_x).mean()\n",
        "    bdry_sample, outward_direction = rho_bdry_sampler_with_directional_vector(2)\n",
        "    psi_x = net_psi(bdry_sample)\n",
        "    grad_u = gradient_nn(net_u, bdry_sample)\n",
        "    directional_grad_u = torch.sum(outward_direction * grad_u, -1).unsqueeze(-1)\n",
        "    bd_loss = (directional_grad_u * psi_x).mean()\n",
        "    return in_loss + bd_lambda * bd_loss\n",
        "\n",
        "\n",
        "# PDHG loss with extrapolation, i.e., replace phi in PDHG_loss by (1+\\omega) * phi_k+1 - \\omega * phi_k\n",
        "#                                     replace psi in PDHG_loss by (1+\\omega) * psi_k+1 - \\omega * psi_k\n",
        "def PDHG_loss_with_extraplt(net_u, u_laststep, net_phi_1, net_phi_0, net_psi_1, net_psi_0, in_samples, bd_samples, h_t, ave_value_ddW, bd_lambda, omega, a=a, b=b):\n",
        "\n",
        "    u_x = net_u(in_samples)\n",
        "    phi_1_x = net_phi_1(in_samples)\n",
        "    phi_0_x = net_phi_0(in_samples)\n",
        "    tilde_phi_x = phi_1_x + omega * (phi_1_x - phi_0_x)\n",
        "    grad_u = gradient_nn(net_u, in_samples)\n",
        "    grad_phi_1 = gradient_nn(net_phi_1, in_samples)\n",
        "    grad_phi_0 = gradient_nn(net_phi_0, in_samples)\n",
        "    grad_tilde_phi = grad_phi_1 + omega * (grad_phi_1 - grad_phi_0)\n",
        "    dW_u = dW(u_x)\n",
        "    loss1 = ((u_x + b * h_t * dW_u - u_laststep)  * tilde_phi_x).mean()\n",
        "    loss2 = a * h_t * (torch.sum(grad_u * grad_tilde_phi, -1).unsqueeze(-1)).mean()\n",
        "    in_loss = loss1 + loss2\n",
        "\n",
        "    bdry_sample, outward_direction = rho_bdry_sampler_with_directional_vector(2)\n",
        "    psi_1_x = net_psi_1(bdry_sample)\n",
        "    psi_0_x = net_psi_0(bdry_sample)\n",
        "    tilde_psi_x = psi_1_x + omega * (psi_1_x - psi_0_x)\n",
        "    grad_u = gradient_nn(net_u, bdry_sample)\n",
        "    directional_grad_u = torch.sum(outward_direction * grad_u, -1).unsqueeze(-1)\n",
        "    bd_loss = (directional_grad_u * tilde_psi_x).mean()\n",
        "\n",
        "    return in_loss + bd_lambda * bd_loss\n",
        "\n",
        "\n",
        "def PDHG_loss_with_extraplt_typePINN(net_u, u_laststep, net_phi_1, net_phi_0, net_psi_1, net_psi_0, in_samples, bd_samples, h_t, ave_value_ddW, bd_lambda, omega, a=a, b=b):\n",
        "\n",
        "    u_x = net_u(in_samples)\n",
        "    phi_1_x = net_phi_1(in_samples)\n",
        "    phi_0_x = net_phi_0(in_samples)\n",
        "    tilde_phi_x = phi_1_x + omega * (phi_1_x - phi_0_x)\n",
        "\n",
        "    Lap_u = v_compute_Laplacian(net_u, in_samples)\n",
        "    dW_u = dW(u_x)\n",
        "    in_loss = ((u_x + h_t * (-a * Lap_u + b * dW_u) - u_laststep) * tilde_phi_x).mean()\n",
        "\n",
        "\n",
        "    bdry_sample, outward_direction = rho_bdry_sampler_with_directional_vector(2)\n",
        "    psi_1_x = net_psi_1(bdry_sample)\n",
        "    psi_0_x = net_psi_0(bdry_sample)\n",
        "    tilde_psi_x = psi_1_x + omega * (psi_1_x - psi_0_x)\n",
        "    grad_u = gradient_nn(net_u, bdry_sample)\n",
        "    directional_grad_u = torch.sum(outward_direction * grad_u, -1).unsqueeze(-1)\n",
        "    bd_loss = (directional_grad_u * tilde_psi_x).mean()\n",
        "\n",
        "    return in_loss + bd_lambda * bd_loss\n",
        "\n",
        "\n",
        "def L2_norm_sq_phi(net_phi, samples):\n",
        "    phi_samples = net_phi(samples)\n",
        "    norm_sq = (phi_samples * phi_samples).mean()\n",
        "    return norm_sq\n",
        "\n",
        "\n",
        "def L2_norm_sq_Lap_phi(net_phi, samples):\n",
        "    lap_phi = v_compute_Laplacian(net_phi, samples)\n",
        "    norm_sq = torch.sum(lap_phi * lap_phi, -1).mean()\n",
        "    return norm_sq\n",
        "\n",
        "\n",
        "def L2_norm_sq_nabla_phi(net_phi, samples):\n",
        "    grad_phi = gradient_nn(net_phi, samples)\n",
        "    norm_sq = torch.sum(grad_phi * grad_phi, -1).mean()\n",
        "    return norm_sq\n",
        "\n",
        "\n",
        "def L2_norm_D_phi(net_phi, samples, h_t, ave_value_ddW, a=a, b=b):\n",
        "    phi_x = net_phi(samples)\n",
        "    phi_sqr = (phi_x * phi_x).mean()\n",
        "    grad_phi = gradient_nn(net_phi, samples)\n",
        "    grad_phi_sq = torch.sum(grad_phi, -1).mean()\n",
        "    return (1 + b * h_t * ave_value_ddW) * phi_sqr + a * h_t * grad_phi_sq\n",
        "\n",
        "\n",
        "def L2_norm_sq_psi(net_psi, samples):\n",
        "    psi_samples = net_psi(samples)\n",
        "    norm_sq = (psi_samples * psi_samples).mean()\n",
        "    return norm_sq\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Z-pSeLWCooR8"
      },
      "outputs": [],
      "source": [
        "# @title boundary loss\n",
        "\n",
        "\n",
        "def Bd_loss(net_u, N):\n",
        "    bdry_sample, outward_direction = rho_bdry_sampler_with_directional_vector(N)\n",
        "    grad_u = gradient_nn(net_u, bdry_sample)\n",
        "    directional_grad_u = torch.sum(outward_direction * grad_u, -1).unsqueeze(-1)\n",
        "    bd_loss = (directional_grad_u**2).mean()\n",
        "    return bd_loss\n",
        "\n",
        "\n",
        "def Loss_2(net_u, N):\n",
        "    bd_samples = rho_bdry_sampler(N)\n",
        "    num_u = net_u(bd_samples)\n",
        "    diff_u_real = num_u\n",
        "    loss = (diff_u_real * diff_u_real).mean()\n",
        "    return loss\n",
        "\n",
        "\n",
        "def Bd_loss_Neumann_use_samples(net_u, bd_samples, outward_direction):\n",
        "    grad_u = gradient_nn(net_u, bd_samples)\n",
        "    directional_grad_u = torch.sum(outward_direction * grad_u, -1).unsqueeze(-1)\n",
        "    bd_loss = (directional_grad_u**2).mean()\n",
        "    return bd_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZcvnDLPmjC-M"
      },
      "outputs": [],
      "source": [
        "# @title PINN loss\n",
        "import torch.autograd.functional as functional\n",
        "\n",
        "\n",
        "# PINN loss for Laplace equ\n",
        "def PINN_Loss(net_u, u_laststep, in_samples, h_t, bd_lambda, a=a, b=b):\n",
        "    u_x = net_u(in_samples)\n",
        "    Lap_u_x = v_compute_Laplacian(net_u, in_samples)\n",
        "    dW_u = dW(u_x)\n",
        "    diff = (u_x - u_laststep - a * h_t * Lap_u_x + b * h_t * dW_u)#/h_t\n",
        "    residual = (diff * diff).mean()\n",
        "    bdry_samples, outward_direction = rho_bdry_sampler_with_directional_vector(2)\n",
        "    grad_u = gradient_nn(net_u, bdry_samples)\n",
        "    directional_grad_u = torch.sum(outward_direction * grad_u, -1).unsqueeze(-1)\n",
        "    bd_loss = (directional_grad_u**2).mean()\n",
        "    PINNloss =  residual + bd_lambda * bd_loss\n",
        "    return PINNloss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FVbj7yYdsqVz"
      },
      "outputs": [],
      "source": [
        "# @title G(\\theta) as a linear opt another\n",
        "import scipy\n",
        "from scipy.sparse.linalg import LinearOperator\n",
        "\n",
        "\n",
        "def tensor_to_numpy(u):\n",
        "    if u.device==\"cpu\":\n",
        "        return u.detach().numpy()\n",
        "    else:\n",
        "        return u.cpu().detach().numpy()\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "# In this document, we define various forms of the precondition matrix M(\\theta),\n",
        "# matrix M(\\theta) can be viewed as a \"metric tensor\" in the parameter space,\n",
        "# we denote the precondition matrix as \"G\" throughout the implementation.\n",
        "#################################################################################\n",
        "\n",
        "# explicitly form the Gram matrix M(\\theta). Only for verification.\n",
        "def form_metric_tensor(input_dim, net, G_samples, device):\n",
        "    N = G_samples.size()[0]\n",
        "    Jacobi_NN = jacrev(functional_call, argnums = 1)\n",
        "    D_param_D_x_NN = vmap(jacrev(Jacobi_NN, argnums = 2), in_dims = (None, None, 0))\n",
        "    D_param_D_x_net_on_x = D_param_D_x_NN(net, dict(net.named_parameters()), G_samples)\n",
        "    num_params = torch.nn.utils.parameters_to_vector(net.parameters()).size()[0]\n",
        "    print(\"Number of params = {}\".format(num_params))\n",
        "    list_of_vectorized_param_gradients = []\n",
        "    for param_gradients in dict(D_param_D_x_net_on_x).items():\n",
        "        vectorized_param_gradients = param_gradients[1].view(N, -1, input_dim)\n",
        "        list_of_vectorized_param_gradients.append(vectorized_param_gradients)\n",
        "    total_vectorized_param_gradients = torch.cat(list_of_vectorized_param_gradients, 1)\n",
        "    transpose_total_vectorized_param_gradients = torch.transpose(total_vectorized_param_gradients, 1, 2)\n",
        "    batched_metric_tensor = torch.matmul(total_vectorized_param_gradients, transpose_total_vectorized_param_gradients)\n",
        "    metric_tensor = torch.mean(batched_metric_tensor, 0)\n",
        "    return metric_tensor\n",
        "\n",
        "\n",
        "def metric_tensor_as_Laplace_op(net, net_auxil, G_samples, vec, device):\n",
        "    num_params = len(torch.nn.utils.parameters_to_vector(net.parameters()))\n",
        "    params_net = dict(net.named_parameters())\n",
        "    params_net_auxil = dict(net_auxil.named_parameters())\n",
        "    ################### computation starts here ##################################\n",
        "    net.zero_grad()\n",
        "    net_auxil.zero_grad()\n",
        "    laplace_net_x = v_compute_Laplacian(net, G_samples)\n",
        "    laplace_net_auxil_x = v_compute_Laplacian(net_auxil, G_samples)\n",
        "    ave_sqr_laplace_net = torch.sum(laplace_net_x * laplace_net_auxil_x) / G_samples.size()[0]\n",
        "    nabla_theta_ave_sqr_laplace_net = torch.autograd.grad(ave_sqr_laplace_net, net_auxil.parameters(), grad_outputs=None, allow_unused=True, retain_graph=True, create_graph=True)\n",
        "    vectorize_nabla_theta_ave_sqr_laplace_net = torch.nn.utils.parameters_to_vector(nabla_theta_ave_sqr_laplace_net)\n",
        "    vec_dot_nabla_theta_ave_sqr_laplace_net = vectorize_nabla_theta_ave_sqr_laplace_net.dot(vec)\n",
        "    metric_tensor_mult_vec = torch.autograd.grad(vec_dot_nabla_theta_ave_sqr_laplace_net, net.parameters(), grad_outputs=None, allow_unused=True, retain_graph=True, create_graph=True)\n",
        "    vectorize_metric_tensor_mult_vec = torch.nn.utils.parameters_to_vector(metric_tensor_mult_vec)\n",
        "    return vectorize_metric_tensor_mult_vec\n",
        "\n",
        "\n",
        "def metric_tensor_as_nabla_op(net, net_auxil, G_samples, vec, device):\n",
        "    num_params = len(torch.nn.utils.parameters_to_vector(net.parameters()))\n",
        "    params_net = dict(net.named_parameters())\n",
        "    params_net_auxil = dict(net_auxil.named_parameters())\n",
        "    ################### computation starts here ##################################\n",
        "    net.zero_grad()\n",
        "    net_auxil.zero_grad()\n",
        "    grad_net_x = gradient_nn(net, G_samples)\n",
        "    grad_net_auxil_x = gradient_nn(net_auxil, G_samples)\n",
        "    ave_sqr_grad_net = torch.sum(grad_net_x * grad_net_auxil_x) / G_samples.size()[0]  # torch.sum(grad_net_x * grad_net_auxil_x, 2).mean()\n",
        "    nabla_theta_ave_sqr_grad_net = torch.autograd.grad(ave_sqr_grad_net, net_auxil.parameters(), grad_outputs=None ,allow_unused=True, retain_graph=True, create_graph=True)\n",
        "    vectorize_nabla_theta_ave_sqr_grad_net = torch.nn.utils.parameters_to_vector(nabla_theta_ave_sqr_grad_net)\n",
        "    vec_dot_nabla_theta_ave_sqr_grad_net = vectorize_nabla_theta_ave_sqr_grad_net.dot(vec)\n",
        "    metric_tensor_mult_vec = torch.autograd.grad(vec_dot_nabla_theta_ave_sqr_grad_net, net.parameters(), grad_outputs=None,allow_unused=True, retain_graph=True, create_graph=True)\n",
        "    vectorize_metric_tensor_mult_vec = torch.nn.utils.parameters_to_vector(metric_tensor_mult_vec)\n",
        "    return vectorize_metric_tensor_mult_vec\n",
        "\n",
        "\n",
        "def metric_tensor_as_op_identity_part(net, net_auxil, G_samples, vec, device):\n",
        "    num_params = len(torch.nn.utils.parameters_to_vector(net.parameters()))\n",
        "    params_net = dict(net.named_parameters())\n",
        "    params_net_auxil = dict(net_auxil.named_parameters())\n",
        "    ################### computation starts here ##################################\n",
        "    net_x = net(G_samples)\n",
        "    net_auxil_x = net_auxil(G_samples)\n",
        "    ave_net = torch.sum(net_x * net_auxil_x) / G_samples.size()[0]\n",
        "    nabla_theta_ave_net = torch.autograd.grad(ave_net, net_auxil.parameters(), grad_outputs=None ,allow_unused=True, retain_graph=True, create_graph=True)\n",
        "    vectorize_nabla_theta_net = torch.nn.utils.parameters_to_vector(nabla_theta_ave_net)\n",
        "    vec_dot_nabla_theta_ave_net = vectorize_nabla_theta_net.dot(vec)\n",
        "    metric_tensor_mult_vec = torch.autograd.grad(vec_dot_nabla_theta_ave_net, net.parameters(), grad_outputs=None,allow_unused=True, retain_graph=True, create_graph=True)\n",
        "    vectorize_metric_tensor_mult_vec = torch.nn.utils.parameters_to_vector(metric_tensor_mult_vec)\n",
        "    return vectorize_metric_tensor_mult_vec\n",
        "\n",
        "\n",
        "# \\mathcal M_p = Id + h_t * W''(u) operator\n",
        "def metric_tensor_as_op_DF_part(net, net_auxil, net_auxil_auxil, G_samples, vec, device):\n",
        "    num_params = len(torch.nn.utils.parameters_to_vector(net.parameters()))\n",
        "    params_net = dict(net.named_parameters())\n",
        "    params_net_auxil = dict(net_auxil.named_parameters())\n",
        "    ################### computation starts here ##################################\n",
        "    net_auxil_auxil_x = net_auxil_auxil(G_samples)\n",
        "    DFu_net = net_auxil_auxil_x + b * h_t * ddW(net_auxil_auxil_x)\n",
        "    net_x = net(G_samples)\n",
        "    net_auxil_x = net_auxil(G_samples)\n",
        "    ave_net = torch.sum((DFu_net * net_x) * (DFu_net * net_auxil_x)) / G_samples.size()[0]\n",
        "    nabla_theta_ave_net = torch.autograd.grad(ave_net, net_auxil.parameters(), grad_outputs=None ,allow_unused=True, retain_graph=True, create_graph=True)\n",
        "    vectorize_nabla_theta_net = torch.nn.utils.parameters_to_vector(nabla_theta_ave_net)\n",
        "    vec_dot_nabla_theta_ave_net = vectorize_nabla_theta_net.dot(vec)\n",
        "    metric_tensor_mult_vec = torch.autograd.grad(vec_dot_nabla_theta_ave_net, net.parameters(), grad_outputs=None,allow_unused=True, retain_graph=True, create_graph=True)\n",
        "    vectorize_metric_tensor_mult_vec = torch.nn.utils.parameters_to_vector(metric_tensor_mult_vec)\n",
        "    return vectorize_metric_tensor_mult_vec\n",
        "\n",
        "\n",
        "def metric_tensor_as_trace_op(net, net_auxil, G_samples, vec, device):\n",
        "    num_params = len(torch.nn.utils.parameters_to_vector(net.parameters()))\n",
        "    params_net = dict(net.named_parameters())\n",
        "    params_net_auxil = dict(net_auxil.named_parameters())\n",
        "    ################### computation starts here ##################################\n",
        "    net_x = net(G_samples)\n",
        "    net_auxil_x = net_auxil(G_samples)\n",
        "    ave_net = torch.sum(net_x * net_auxil_x) / G_samples.size()[0]\n",
        "    nabla_theta_ave_net = torch.autograd.grad(ave_net, net_auxil.parameters(), grad_outputs=None ,allow_unused=True, retain_graph=True, create_graph=True)\n",
        "    vectorize_nabla_theta_net = torch.nn.utils.parameters_to_vector(nabla_theta_ave_net)\n",
        "    vec_dot_nabla_theta_ave_net = vectorize_nabla_theta_net.dot(vec)\n",
        "    metric_tensor_mult_vec = torch.autograd.grad(vec_dot_nabla_theta_ave_net, net.parameters(), grad_outputs=None,allow_unused=True, retain_graph=True, create_graph=True)\n",
        "    vectorize_metric_tensor_mult_vec = torch.nn.utils.parameters_to_vector(metric_tensor_mult_vec)\n",
        "    return vectorize_metric_tensor_mult_vec\n",
        "\n",
        "\n",
        "def metric_tensor_as_Neumann_trace_op(net, net_auxil, G_samples, outward_direction, vec, device):\n",
        "    num_params = len(torch.nn.utils.parameters_to_vector(net.parameters()))\n",
        "    params_net = dict(net.named_parameters())\n",
        "    params_net_auxil = dict(net_auxil.named_parameters())\n",
        "    ################### computation starts here ##################################\n",
        "    net_x = net(G_samples)\n",
        "    grad_net_x = gradient_nn(net, G_samples)\n",
        "    directional_grad_net_x = torch.sum(outward_direction * grad_net_x, -1).unsqueeze(-1)\n",
        "    net_auxil_x = net_auxil(G_samples)\n",
        "    grad_net_auxil_x = gradient_nn(net_auxil, G_samples)\n",
        "    directional_grad_net_auxil_x = torch.sum(outward_direction * grad_net_auxil_x, -1).unsqueeze(-1)\n",
        "    ave_net = torch.sum(directional_grad_net_x * directional_grad_net_auxil_x) / G_samples.size()[0]\n",
        "    nabla_theta_ave_net = torch.autograd.grad(ave_net, net_auxil.parameters(), grad_outputs=None ,allow_unused=True, retain_graph=True, create_graph=True)\n",
        "    vectorize_nabla_theta_net = torch.nn.utils.parameters_to_vector(nabla_theta_ave_net)\n",
        "    vec_dot_nabla_theta_ave_net = vectorize_nabla_theta_net.dot(vec)\n",
        "    metric_tensor_mult_vec = torch.autograd.grad(vec_dot_nabla_theta_ave_net, net.parameters(), grad_outputs=None,allow_unused=True, retain_graph=True, create_graph=True)\n",
        "    vectorize_metric_tensor_mult_vec = torch.nn.utils.parameters_to_vector(metric_tensor_mult_vec)\n",
        "    return vectorize_metric_tensor_mult_vec\n",
        "\n",
        "\n",
        "# G1: \\mathcal M = Id operator\n",
        "# G2: \\mathcal M = ▽ operator\n",
        "# G3: \\mathcal M = -△ operator\n",
        "# G4: \\mathcal M = lambda T (Trace) operator\n",
        "# G5: \\mathcal M: u \\mapsto \\partial u \\partial n on \\partial\\Omega\n",
        "# G6: \\mathcal M: u \\mapsto (I + h_t*W''(u))u\n",
        "\n",
        "# In this implementation,\n",
        "# as epsilon_0 = 0.01,\n",
        "# we use G65 for u, G1 for \\phi, G4 for \\psi\n",
        "# in the current paper.\n",
        "def minres_solver_G(net, net_auxil, interior_samples, boundary_samples, outward_direction, RHS_vec, device, bd_lambda, max_iternum, minres_tolerance, G_type, ave_value_ddW, h_t=0.1, a=a, b=b, net_auxil_auxil=0):\n",
        "\n",
        "    num_params = torch.nn.utils.parameters_to_vector(net.parameters()).size()[0]\n",
        "\n",
        "    def G1_as_operator(vec):  # input the vector v [on CPU], return vector Gv\n",
        "        tensorized_vec = torch.Tensor(vec).to(device)\n",
        "        Gv = metric_tensor_as_op_identity_part(net, net_auxil, interior_samples, tensorized_vec, device)\n",
        "        return tensor_to_numpy(Gv)\n",
        "\n",
        "    def G2_as_operator(vec):  # input the vector v [on CPU], return vector Gv\n",
        "        tensorized_vec = torch.Tensor(vec).to(device)\n",
        "        Gv = metric_tensor_as_nabla_op(net, net_auxil, interior_samples, tensorized_vec, device)\n",
        "        return tensor_to_numpy(Gv)\n",
        "\n",
        "    def G3_as_operator(vec):  # input the vector v [on CPU], return vector Gv\n",
        "        tensorized_vec = torch.Tensor(vec).to(device)\n",
        "        Gv = metric_tensor_as_Laplace_op(net, net_auxil, interior_samples, tensorized_vec, device)\n",
        "        return tensor_to_numpy(Gv)\n",
        "\n",
        "    def G4_as_operator(vec):  # input the vector v [on CPU], return vector Gv\n",
        "        tensorized_vec = torch.Tensor(vec).to(device)\n",
        "        Gv = metric_tensor_as_trace_op(net, net_auxil, boundary_samples, tensorized_vec, device)\n",
        "        return tensor_to_numpy(bd_lambda * Gv)\n",
        "\n",
        "    def G5_as_operator(vec):\n",
        "        tensorized_vec = torch.Tensor(vec).to(device)\n",
        "        Gv = metric_tensor_as_Neumann_trace_op(net, net_auxil, boundary_samples, outward_direction, tensorized_vec, device)\n",
        "        return tensor_to_numpy(bd_lambda * Gv)\n",
        "\n",
        "    def G6_as_operator(vec):\n",
        "        tensorized_vec = torch.Tensor(vec).to(device)\n",
        "        Gv = metric_tensor_as_op_DF_part(net, net_auxil, net_auxil_auxil, interior_samples, tensorized_vec, device)\n",
        "        return tensor_to_numpy(Gv)\n",
        "\n",
        "    def G14_as_operator(vec):  # input the vector v [on CPU], return vector Gv\n",
        "        tensorized_vec = torch.Tensor(vec).to(device)\n",
        "        Gv = metric_tensor_as_op_identity_part(net, net_auxil, interior_samples, tensorized_vec, device) + bd_lambda * metric_tensor_as_trace_op(net, net_auxil, boundary_samples, tensorized_vec, device)\n",
        "        return tensor_to_numpy(Gv)\n",
        "\n",
        "    def G24_as_operator(vec):  # input the vector v [on CPU], return vector Gv\n",
        "        tensorized_vec = torch.Tensor(vec).to(device)\n",
        "        Gv = metric_tensor_as_nabla_op(net, net_auxil, interior_samples, tensorized_vec, device) + bd_lambda * metric_tensor_as_trace_op(net, net_auxil, boundary_samples, tensorized_vec, device)\n",
        "        return tensor_to_numpy(Gv)\n",
        "\n",
        "    def G65_RD_as_operator(vec):\n",
        "        tensorized_vec = torch.Tensor(vec).to(device)\n",
        "        Gv = metric_tensor_as_op_DF_part(net, net_auxil, net_auxil_auxil, interior_samples, tensorized_vec, device) + bd_lambda * metric_tensor_as_Neumann_trace_op(net, net_auxil, boundary_samples, outward_direction, tensorized_vec, device)\n",
        "        return tensor_to_numpy(Gv)\n",
        "\n",
        "    def G125_RD_as_operator(vec):\n",
        "        tensorized_vec = torch.Tensor(vec).to(device)\n",
        "        Gv =  (1+b*h_t*ave_value_ddW) * metric_tensor_as_op_identity_part(net, net_auxil, interior_samples, tensorized_vec, device) + a * h_t * metric_tensor_as_nabla_op(net, net_auxil, interior_samples, tensorized_vec, device) + bd_lambda * metric_tensor_as_Neumann_trace_op(net, net_auxil, boundary_samples, outward_direction, tensorized_vec, device)\n",
        "        return tensor_to_numpy(Gv)\n",
        "\n",
        "    def G12_RD_as_operator(vec):\n",
        "        tensorized_vec = torch.Tensor(vec).to(device)\n",
        "        Gv = (1+b*h_t*ave_value_ddW) * metric_tensor_as_op_identity_part(net, net_auxil, interior_samples, tensorized_vec, device) + a * h_t * metric_tensor_as_nabla_op(net, net_auxil, interior_samples, tensorized_vec, device)\n",
        "        return tensor_to_numpy(Gv)\n",
        "\n",
        "    if G_type == \"1\":\n",
        "        G_operator = LinearOperator((num_params, num_params), matvec=G1_as_operator)\n",
        "    elif G_type == \"2\":\n",
        "        G_operator = LinearOperator((num_params, num_params), matvec=G2_as_operator)\n",
        "    elif G_type == \"3\":\n",
        "        G_operator = LinearOperator((num_params, num_params), matvec=G3_as_operator)\n",
        "    elif G_type == \"4\":\n",
        "        G_operator = LinearOperator((num_params, num_params), matvec=G4_as_operator)\n",
        "    elif G_type == \"5\":\n",
        "        G_operator = LinearOperator((num_params, num_params), matvec=G5_as_operator)\n",
        "    elif G_type == \"14\":\n",
        "        G_operator = LinearOperator((num_params, num_params), matvec=G14_as_operator)\n",
        "    elif G_type == \"24\":\n",
        "        G_operator = LinearOperator((num_params, num_params), matvec=G24_as_operator)\n",
        "    elif G_type == \"65\":\n",
        "        G_operator = LinearOperator((num_params, num_params), matvec=G65_RD_as_operator)\n",
        "    elif G_type == \"125\":\n",
        "        G_operator = LinearOperator((num_params, num_params), matvec=G125_RD_as_operator)\n",
        "    elif G_type == \"12\":\n",
        "        G_operator = LinearOperator((num_params, num_params), matvec=G12_RD_as_operator)\n",
        "    else:\n",
        "        print(\"Wrong G_type\")\n",
        "\n",
        "    np_RHS_vec = tensor_to_numpy(RHS_vec)\n",
        "    sol_vec, info = scipy.sparse.linalg.minres(G_operator, np_RHS_vec, rtol=minres_tolerance, maxiter=max_iternum)\n",
        "    if (torch.max(torch.isnan(torch.tensor(sol_vec))) > 0):\n",
        "        print(\"Got the NAN!!!\")\n",
        "        sol_vec = np_RHS_vec\n",
        "        info = 0\n",
        "    tensorized_sol_vec = torch.Tensor(sol_vec).to(device)\n",
        "\n",
        "    return tensorized_sol_vec, info\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5kyfJ8Mpd7w7"
      },
      "outputs": [],
      "source": [
        "# @title update param via line search\n",
        "\n",
        "def update_param(number_stepsizes, base_stepsize, theta_0, tangent_theta, net_u, net_phi, net_psi, in_samples, bd_samples, bd_lambda, descent_or_ascent, net_type, epsilon, loss_phi_or_psi=None):\n",
        "\n",
        "    stepsize_list = 0.2 * base_stepsize**np.arange(number_stepsizes)\n",
        "    min_index = 0\n",
        "    index = 0\n",
        "    loss_along_stepsizes = []\n",
        "    current_min = 1000\n",
        "\n",
        "    if descent_or_ascent == \"descent\":\n",
        "        flag = -1\n",
        "    else:\n",
        "        flag = 1\n",
        "\n",
        "    if net_type == \"u\":\n",
        "        net_test = network_prim(network_length, dim, hidden_dimension_net_u, 1).to(device)\n",
        "    elif net_type == \"phi\":\n",
        "        net_test = network_dual(network_length, dim, hidden_dimension_net_phi, 1, L).to(device)\n",
        "    elif net_type == \"psi\":\n",
        "        net_test = network_dual_on_bdry(network_length, dim, hidden_dimension_net_psi, 1).to(device)\n",
        "    else:\n",
        "        print(\"Wrong net_type\")\n",
        "\n",
        "    for stepsize in stepsize_list:\n",
        "        updated_theta = theta_0 + flag * stepsize * tangent_theta\n",
        "\n",
        "        torch.nn.utils.vector_to_parameters(updated_theta, net_test.parameters())\n",
        "        pdhgloss = PDHG_loss(net_u, net_phi, net_psi, in_samples, bd_samples, bd_lambda)\n",
        "        if net_type == \"u\":\n",
        "            loss_regularization = 0\n",
        "        elif net_type == \"phi\":\n",
        "            loss_regularization = loss_phi_or_psi(net_phi, in_samples)\n",
        "        elif net_type == \"psi\":\n",
        "            loss_regularization = loss_phi_or_psi(net_psi, bd_samples)\n",
        "\n",
        "        loss = pdhgloss - epsilon / 2 * loss_regularization\n",
        "\n",
        "        loss_along_stepsizes.append( - flag * loss.cpu().detach().numpy() )\n",
        "        if loss < current_min:\n",
        "            min_index = index\n",
        "            current_min = loss\n",
        "\n",
        "        index = index + 1\n",
        "\n",
        "    optimal_stepsize = stepsize_list[min_index]\n",
        "    optimal_updated_theta = theta_0 + flag * optimal_stepsize * tangent_theta\n",
        "\n",
        "    return optimal_updated_theta, optimal_stepsize, loss_along_stepsizes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FcGRXFBphnn"
      },
      "outputs": [],
      "source": [
        "# @title NPDHG solver on a single time interval\n",
        "import pickle\n",
        "\n",
        "\n",
        "def NPDHG_solver_on_single_time_interval(device, save_path, net_u_laststep, phy_time, h_t, L, ave_value_ddW, N_r, N_b,\n",
        "                minres_max_iter, minres_tol,\n",
        "                network_length, hidden_dimension_net_u, hidden_dimension_net_phi, hidden_dimension_net_psi, flag_init,\n",
        "                iter, phi_psi_iter, u_iter, omega, epsilon,\n",
        "                plot_period, print_period, N_plot, chosen_dim_0, chosen_dim_1, flag_plot_real, u_real, N_IMEX,\n",
        "                number_stepsizes, base_stepsize,\n",
        "                precond_type,\n",
        "                bd_lambda,\n",
        "                stepsize_0=0.2,\n",
        "                adaptive_or_fixed_stepsize =\"fixed\", tau_u = 0.5 * 1e-1, tau_phi = 0.9 * 1e-1, tau_psi = 0.9 * 1e-1):\n",
        "\n",
        "    torch.manual_seed(50)\n",
        "\n",
        "    # initialize nets\n",
        "    net_u = network_prim(network_length, dim, hidden_dimension_net_u, 1).to(device)\n",
        "    net_u.load_state_dict(net_u_laststep.state_dict())\n",
        "    net_phi = network_dual(network_length, dim, hidden_dimension_net_phi, 1, L).to(device)\n",
        "    net_psi = network_dual_on_bdry(network_length, dim, hidden_dimension_net_psi, 1).to(device)\n",
        "\n",
        "    if flag_init == True:\n",
        "        net_u.initialization()\n",
        "        net_phi.initialization()\n",
        "        net_psi.initialization()\n",
        "\n",
        "    if precond_type == \"MpMd_Id\":\n",
        "       G_u_type = \"14\"\n",
        "       loss_phi = L2_norm_sq_phi\n",
        "       G_phi_type = \"1\"\n",
        "    elif precond_type == \"MpMd_nabla\":\n",
        "       G_u_type = \"24\"\n",
        "       loss_phi = L2_norm_sq_nabla_phi\n",
        "       G_phi_type = \"2\"\n",
        "    elif precond_type == \"Mp_Id_Md_Laplace\":\n",
        "       G_u_type = \"14\"\n",
        "       loss_phi = L2_norm_sq_Lap_phi\n",
        "       G_phi_type = \"3\"\n",
        "    elif precond_type == \"RD_eps_small\": # use in this test\n",
        "        G_u_type = \"65\"\n",
        "        G_phi_type = \"1\"\n",
        "        loss_phi = L2_norm_sq_phi\n",
        "    elif precond_type == \"RD_eps_large\":\n",
        "        G_u_type = \"125\"\n",
        "        G_phi_type = \"12\"\n",
        "        loss_phi = L2_norm_D_phi\n",
        "    loss_psi = L2_norm_sq_psi\n",
        "    G_psi_type = \"4\"\n",
        "\n",
        "    comp_time = []\n",
        "    total_time = 0\n",
        "    l2error_list = []\n",
        "    H1error_list = []\n",
        "    l2res_list = []\n",
        "    bdryerr_list = []\n",
        "    preconded_nabla_eta_norm_list = []\n",
        "    preconded_nabla_eta2_norm_list = []\n",
        "    preconded_nabla_theta_norm_list = []\n",
        "    ######################################################### PDHG iterations START HERE ###################################################################################################################\n",
        "    for t in range(iter):\n",
        "\n",
        "        t_0 = time.time()\n",
        "\n",
        "        in_samples = rho_1_sampler(N_r)\n",
        "        bd_samples, outward_direction = rho_bdry_sampler_with_directional_vector(N_b)\n",
        "        u_laststep = net_u_laststep(in_samples)\n",
        "\n",
        "        net_u.zero_grad()\n",
        "        net_phi.zero_grad()\n",
        "        net_psi.zero_grad()\n",
        "        ############################# update phi_\\eta #####################################\n",
        "        original_eta = torch.nn.utils.parameters_to_vector(net_phi.parameters())\n",
        "        original_eta2 = torch.nn.utils.parameters_to_vector(net_psi.parameters())\n",
        "        for inner_iter in range(phi_psi_iter):\n",
        "            ############### compute G(\\eta)^{-1} \\nabla_\\eta loss() #########################\n",
        "            lossa = PDHG_loss_typePINN(net_u, u_laststep, net_phi, net_psi, in_samples, bd_samples, h_t, ave_value_ddW, bd_lambda) - epsilon/2 * loss_phi(net_phi, in_samples)\n",
        "            nabla_eta_loss = torch.autograd.grad(lossa, net_phi.parameters(), grad_outputs=None, allow_unused=True, retain_graph=True, create_graph=True)\n",
        "            vectorized_nabla_eta_loss = torch.nn.utils.parameters_to_vector(nabla_eta_loss)\n",
        "\n",
        "            # copy net_phi for G(\\eta) computation\n",
        "            net_phi_auxil = network_dual(network_length, dim, hidden_dimension_net_phi, 1, L).to(device)\n",
        "            net_phi_auxil.load_state_dict(net_phi.state_dict())\n",
        "\n",
        "            # compute G(\\eta)^{-1} \\nabla_\\eta loss()\n",
        "            G_inv_nabla_eta_loss, info_phi = minres_solver_G(net_phi, net_phi_auxil, in_samples, bd_samples, outward_direction, vectorized_nabla_eta_loss, device, bd_lambda, minres_max_iter, minres_tol, G_phi_type, ave_value_ddW, h_t)\n",
        "\n",
        "            ############### compute G(\\eta2)^{-1} \\nabla_\\eta2 loss() #########################\n",
        "            lossa2 = PDHG_loss_typePINN(net_u, u_laststep, net_phi, net_psi, in_samples, bd_samples, h_t, ave_value_ddW, bd_lambda) - epsilon/2 * bd_lambda * loss_psi(net_psi, bd_samples)\n",
        "            nabla_eta2_loss = torch.autograd.grad(lossa2, net_psi.parameters(), grad_outputs=None, allow_unused=True, retain_graph=True, create_graph=True)\n",
        "            vectorized_nabla_eta2_loss = torch.nn.utils.parameters_to_vector(nabla_eta2_loss)\n",
        "\n",
        "            # copy net_phi for G(\\eta2) computation\n",
        "            net_psi_auxil = network_dual_on_bdry(network_length, dim, hidden_dimension_net_psi, 1).to(device)\n",
        "            net_psi_auxil.load_state_dict(net_psi.state_dict())\n",
        "\n",
        "            # compute G(\\eta2)^{-1} \\nabla_\\eta2 loss()\n",
        "            G_inv_nabla_eta2_loss, info_psi = minres_solver_G(net_psi, net_psi_auxil, in_samples, bd_samples, outward_direction, vectorized_nabla_eta2_loss, device, bd_lambda, minres_max_iter, minres_tol, G_psi_type, ave_value_ddW, h_t)\n",
        "\n",
        "            # update \\eta and \\eta2\n",
        "            original_eta = torch.nn.utils.parameters_to_vector(net_phi.parameters())\n",
        "            original_eta2 = torch.nn.utils.parameters_to_vector(net_psi.parameters())\n",
        "            if adaptive_or_fixed_stepsize == \"adaptive\":\n",
        "               updated_eta, tau_phi, value_along_tau_phis = update_param(number_stepsizes, base_stepsize, original_eta, G_inv_nabla_eta_loss, net_u, net_phi, net_psi, in_samples, bd_samples, bd_lambda, \"ascent\", \"phi\", epsilon, loss_phi)\n",
        "               updated_eta2, tau_psi, value_along_tau_psis = update_param(number_stepsizes, base_stepsize, original_eta2, G_inv_nabla_eta2_loss, net_u, net_phi, net_psi, in_samples, bd_samples, bd_lambda, \"ascent\", \"psi\", epsilon, loss_psi)\n",
        "            elif adaptive_or_fixed_stepsize == \"fixed\":\n",
        "               updated_eta = original_eta + tau_phi * G_inv_nabla_eta_loss\n",
        "               updated_eta2 = original_eta2 + tau_psi * G_inv_nabla_eta2_loss\n",
        "            else:\n",
        "               raise ValueError(\"adaptive_or_fixed_stepsize must be 'adaptive' or 'fixed'\")\n",
        "            torch.nn.utils.vector_to_parameters(updated_eta, net_phi.parameters())\n",
        "            torch.nn.utils.vector_to_parameters(updated_eta2, net_psi.parameters())\n",
        "\n",
        "        ######################## update theta ##################################\n",
        "        net_phi_0 = network_dual(network_length, dim, hidden_dimension_net_phi, 1, L).to(device)\n",
        "        torch.nn.utils.vector_to_parameters(original_eta, net_phi_0.parameters())\n",
        "        net_psi_0 = network_dual_on_bdry(network_length, dim, hidden_dimension_net_psi, 1).to(device)\n",
        "        torch.nn.utils.vector_to_parameters(original_eta2, net_psi_0.parameters())\n",
        "        for inner_iter in range(u_iter):\n",
        "            # compute G(\\theta)^{-1} \\nabla_\\theta loss()\n",
        "            loss_pinn = PINN_Loss(net_u, u_laststep, in_samples, h_t, bd_lambda)\n",
        "            loss_pd = PDHG_loss_with_extraplt_typePINN(net_u, u_laststep, net_phi, net_phi_0, net_psi , net_psi_0, in_samples, bd_samples, h_t, ave_value_ddW, bd_lambda, omega)\n",
        "            lossb = 1 * loss_pd + loss_pinn\n",
        "            nabla_theta_loss = torch.autograd.grad(lossb, net_u.parameters(), grad_outputs=None, allow_unused=True, retain_graph=True, create_graph=True)\n",
        "            vectorized_nabla_theta_loss = torch.nn.utils.parameters_to_vector(nabla_theta_loss)\n",
        "\n",
        "            # copy net_u for G(\\theta) computation\n",
        "            net_u_auxil = network_prim(network_length, dim, hidden_dimension_net_u, 1).to(device)\n",
        "            net_u_auxil.load_state_dict(net_u.state_dict())\n",
        "            net_u_auxil_auxil = network_prim(network_length, dim, hidden_dimension_net_u, 1).to(device)\n",
        "            net_u_auxil_auxil.load_state_dict(net_u.state_dict())\n",
        "            # compute G(\\theta)^{-1} \\nabla_\\theta loss()\n",
        "            G_inv_nabla_theta_loss, info_u = minres_solver_G(net_u, net_u_auxil, in_samples, bd_samples, outward_direction, vectorized_nabla_theta_loss, device, bd_lambda, minres_max_iter, minres_tol, G_u_type, ave_value_ddW, h_t, net_auxil_auxil=net_u_auxil_auxil)\n",
        "\n",
        "            ############# update theta ####################\n",
        "            original_theta = torch.nn.utils.parameters_to_vector(net_u.parameters())\n",
        "            updated_theta = original_theta - tau_u * G_inv_nabla_theta_loss\n",
        "            torch.nn.utils.vector_to_parameters(updated_theta, net_u.parameters())\n",
        "\n",
        "        t_1 = time.time()\n",
        "        total_time = total_time + (t_1 - t_0)\n",
        "        comp_time.append(total_time)\n",
        "\n",
        "        ################ plot ##################\n",
        "        if (t+1) % plot_period == 0:\n",
        "            z_min = -2.4\n",
        "            z_max = 1.5\n",
        "            Plot_graph_nn_primal(phy_time, net_u, L, N_plot, t, flag_plot_real, u_real, save_path, z_min, z_max, device, chosen_dim_0, chosen_dim_1)\n",
        "\n",
        "        ########################################\n",
        "        L2error = L2_error(net_u, u_real, N_IMEX)\n",
        "        l2error_list.append(L2error.cpu().detach())\n",
        "        bd_samples_for_loss, outward_direction_for_loss = rho_bdry_sampler_with_directional_vector(2000)\n",
        "        boundary_error = torch.sqrt(Bd_loss_Neumann_use_samples(net_u, bd_samples_for_loss, outward_direction_for_loss))\n",
        "        bdryerr_list.append(boundary_error.cpu().detach())\n",
        "        if (t+1) % print_period == 0:\n",
        "            print(\"Iter: {}, \".format(t))\n",
        "            print(\"L2 error = {}\".format(L2error))\n",
        "            print(\"boundary loss = {}\".format(boundary_error))\n",
        "        if L2error < 0.5 * 1e-3:\n",
        "          break\n",
        "\n",
        "    ######################################################### PDHG iterations END HERE #################################################################################################################\n",
        "    # save the models\n",
        "    save_path = os.getcwd()\n",
        "    filename = os.path.join(save_path, 'time={} netu.pt'.format(phy_time))\n",
        "    torch.save(net_u.state_dict(), filename)\n",
        "\n",
        "    save_path = os.getcwd()\n",
        "    filename = os.path.join(save_path, 'time={} netphi.pt'.format(phy_time))\n",
        "    torch.save(net_phi.state_dict(), filename)\n",
        "\n",
        "    save_path = os.getcwd()\n",
        "    filename = os.path.join(save_path, 'time={} netpsi.pt'.format(phy_time))\n",
        "    torch.save(net_psi.state_dict(), filename)\n",
        "\n",
        "    # write down the error\n",
        "    with open('time={} l2error_list'.format(phy_time), 'wb') as file1:\n",
        "        pickle.dump(l2error_list, file1)\n",
        "    with open('time={} boundary_error'.format(phy_time), 'wb') as file3:\n",
        "        pickle.dump(boundary_error, file3)\n",
        "    with open('time={} comp_time'.format(phy_time), 'wb') as file_x:\n",
        "        pickle.dump(comp_time, file_x)\n",
        "\n",
        "    fig_plot = plt.figure(figsize=(15, 15))\n",
        "    plt.plot(comp_time, np.log(l2error_list) / np.log(10), color=\"blue\")\n",
        "    plt.xlabel(\"Iteration\", fontsize=30)\n",
        "    plt.ylabel(\"L2 error\", fontsize=30)\n",
        "    plt.xticks(fontsize=20)\n",
        "    plt.yticks(fontsize=20)\n",
        "    plt.title(\"plot of log_10(Rel L2 error) vs. computation time\\n\", fontsize=40)\n",
        "    fig_plot.savefig(os.path.join(save_path, \"Plot of the log l2 error vs. comptime.pdf\"))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    return net_u\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CRZr3CSl4UAQ"
      },
      "outputs": [],
      "source": [
        "# @title Compute RD equation on the entire time interval\n",
        "# compute sequentially\n",
        "\n",
        "def NPDHG_solver_on_entire_time_interval(device, save_path, T, h_t, L, ave_value_ddW, N_r, N_b,\n",
        "                              iter_init_training,\n",
        "                              minres_max_iter, minres_tol,\n",
        "                              network_length, hidden_dimension_net_u, hidden_dimension_net_phi, hidden_dimension_net_psi, flag_init,\n",
        "                              iter, phi_psi_iter, u_iter, omega, epsilon,\n",
        "                              plot_period, print_period, N_plot, chosen_dim_0, chosen_dim_1, flag_plot_real, m, N_schm, Iter_num_fixed_pt, # m is the number of subintervals for IMEX scheme\n",
        "                              number_stepsizes, base_stepsize,\n",
        "                              precond_type,\n",
        "                              bd_lambda,\n",
        "                              stepsize_0=0.2,\n",
        "                              adaptive_or_fixed_stepsize =\"fixed\", tau_u = 0.5 * 1e-1, tau_phi = 0.9 * 1e-1, tau_psi = 0.9 * 1e-1):\n",
        "\n",
        "    N_t = int(T/h_t)\n",
        "\n",
        "    # compute the numerical solution by fully implicit scheme (FIS)\n",
        "    ut_implicit_schm = Fixed_pt_solver_1D(T, h_t/m, L, N_schm, Iter_num_fixed_pt )\n",
        "    # # compute the numerical solution by IMEX scheme (not used)\n",
        "    # ut_IMEX = IMEX_solver_1D(T, h_t/m, L, N_schm)\n",
        "\n",
        "\n",
        "    net_u_init = network_prim(network_length, dim, hidden_dimension_net_u, 1).to(device)\n",
        "    optim_u = torch.optim.Adam(net_u_init.parameters(), lr=1e-4)\n",
        "\n",
        "    # train for the initial condition\n",
        "    for k in range(iter_init_training):\n",
        "      optim_u.zero_grad()\n",
        "      u0loss = Initial_loss(net_u_init, 500)\n",
        "      u0loss.backward()\n",
        "      optim_u.step()\n",
        "      print(u0loss)\n",
        "      if u0loss < 1e-4:\n",
        "        break\n",
        "\n",
        "    # NPDHG solver\n",
        "    net_u_laststep = net_u_init\n",
        "    for k in range(N_t):\n",
        "\n",
        "        print(\"-----------------------------------------------------------------------\")\n",
        "        print(\"Solve on interval [{}, {}]\".format(round(k*h_t, 3), round((k+1)*h_t, 3)))\n",
        "        print(\"-----------------------------------------------------------------------\")\n",
        "\n",
        "        # use the solution solved from fully implicit scheme as benchmark\n",
        "        benchmark_u = ut_implicit_schm[k*m+m-1, :].cuda()\n",
        "\n",
        "        phy_time = (k+1) * h_t\n",
        "\n",
        "\n",
        "        net_u = NPDHG_solver_on_single_time_interval(device, save_path, net_u_laststep, phy_time, h_t, L, ave_value_ddW, N_r, N_b,\n",
        "                      minres_max_iter, minres_tol,\n",
        "                      network_length, hidden_dimension_net_u, hidden_dimension_net_phi, hidden_dimension_net_psi, flag_init,\n",
        "                      iter, phi_psi_iter, u_iter, omega, epsilon,\n",
        "                      plot_period, print_period, N_plot, chosen_dim_0, chosen_dim_1, flag_plot_real, benchmark_u, N_schm,\n",
        "                      number_stepsizes, base_stepsize,\n",
        "                      precond_type,\n",
        "                      bd_lambda,\n",
        "                      stepsize_0=0.2, adaptive_or_fixed_stepsize =\"fixed\", tau_u = tau_u, tau_phi = tau_phi, tau_psi = tau_psi )\n",
        "        net_u_laststep = net_u\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "ZzVdFUMEqGjN"
      },
      "outputs": [],
      "source": [
        "# @title  apply NPDHG_solver (with time causality)\n",
        "save_path = os.getcwd()\n",
        "\n",
        "\n",
        "T =  0.002\n",
        "h_t = 0.001\n",
        "L=2\n",
        "\n",
        "ave_value_ddW = 2.0\n",
        "\n",
        "N_r = 2000\n",
        "N_b = 20\n",
        "\n",
        "iter_init_training = 20000\n",
        "\n",
        "minres_max_iter = 800\n",
        "minres_tol = 1e-4\n",
        "\n",
        "network_length = 3\n",
        "hidden_dimension_net_u = 128\n",
        "hidden_dimension_net_phi = 128\n",
        "hidden_dimension_net_psi = 32\n",
        "\n",
        "flag_init = False\n",
        "\n",
        "iter = 6000\n",
        "phi_psi_iter = 1\n",
        "u_iter = 1\n",
        "omega = 1.0\n",
        "epsilon = 1\n",
        "\n",
        "plot_period  = 500\n",
        "print_period = 100\n",
        "N_plot = 100\n",
        "chosen_dim_0 = 0\n",
        "chosen_dim_1 = 0 # not used\n",
        "flag_plot_real = True\n",
        "m = 1 # number of time subintv of discrete scheme\n",
        "N_space_discrt =   100\n",
        "Iter_num_fixed_pt = 8000\n",
        "\n",
        "number_stepsizes = 50\n",
        "base_stepsize = 0.8\n",
        "\n",
        "precond_type = \"RD_eps_small\" # if a is bounded away from 0, use \"RD_eps_large\"\n",
        "\n",
        "bd_lambda = 1\n",
        "\n",
        "NPDHG_solver_on_entire_time_interval(device, save_path, T, h_t, L, ave_value_ddW, N_r, N_b,\n",
        "                          iter_init_training,\n",
        "                          minres_max_iter, minres_tol,\n",
        "                          network_length, hidden_dimension_net_u, hidden_dimension_net_phi, hidden_dimension_net_psi, flag_init,\n",
        "                          iter, phi_psi_iter, u_iter, omega, epsilon,\n",
        "                          plot_period, print_period, N_plot, chosen_dim_0, chosen_dim_1, flag_plot_real, m, N_space_discrt, Iter_num_fixed_pt, # m is the number of subintervals for IMEX scheme\n",
        "                          number_stepsizes, base_stepsize,\n",
        "                          precond_type,\n",
        "                          bd_lambda,\n",
        "                          stepsize_0=0.2,\n",
        "                          adaptive_or_fixed_stepsize =\"fixed\", tau_u = 0.01, tau_phi = 0.02, tau_psi = 0.02)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zM_3OumYeqSf"
      },
      "outputs": [],
      "source": [
        "# @title Compute RD equation on the entire time interval (compute sequentially) Start from check point: time=k_0*ht\n",
        "\n",
        "\n",
        "def NPDHG_solver_on_entire_time_interval_start_from_t0(k_0, net_u_t_0, device, save_path, T, h_t, L, ave_value_ddW, N_r, N_b,\n",
        "                              iter_init_training,\n",
        "                              minres_max_iter, minres_tol,\n",
        "                              network_length, hidden_dimension_net_u, hidden_dimension_net_phi, hidden_dimension_net_psi, flag_init,\n",
        "                              iter, phi_psi_iter, u_iter, omega, epsilon,\n",
        "                              plot_period, print_period, N_plot, chosen_dim_0, chosen_dim_1, flag_plot_real, m, N_schm, Iter_num_fixed_pt, # m is the number of subintervals for IMEX scheme\n",
        "                              number_stepsizes, base_stepsize,\n",
        "                              precond_type,\n",
        "                              bd_lambda,\n",
        "                              stepsize_0=0.2,\n",
        "                              adaptive_or_fixed_stepsize =\"fixed\", tau_u = 0.5 * 1e-1, tau_phi = 0.9 * 1e-1, tau_psi = 0.9 * 1e-1):\n",
        "\n",
        "    N_t = int(T/h_t)\n",
        "\n",
        "    # compute the numerical solution by fully implicit scheme (FIS)\n",
        "    ut_implicit_schm = Fixed_pt_solver_1D(T, h_t/m, L, N_schm, Iter_num_fixed_pt )\n",
        "    # compute the numerical solution by IMEX scheme\n",
        "    ut_IMEX = IMEX_solver_1D(T, h_t/m, L, N_schm)\n",
        "\n",
        "    net_u_init = network_prim(network_length, dim, hidden_dimension_net_u, 1).to(device)\n",
        "    net_u_init.load_state_dict(net_u_t_0.state_dict())\n",
        "    optim_u = torch.optim.Adam(net_u_init.parameters(), lr=1e-4)\n",
        "\n",
        "    # NPDHG solver\n",
        "    net_u_laststep = net_u_init\n",
        "    for k in range(k_0, N_t):\n",
        "\n",
        "        print(\"-----------------------------------------------------------------------\")\n",
        "        print(\"Solve on interval [{}, {}]\".format(round(k*h_t, 3), round((k+1)*h_t, 3)))\n",
        "        print(\"-----------------------------------------------------------------------\")\n",
        "\n",
        "        # use the solution solved from fully implicit scheme as benchmark\n",
        "        benchmark_u = ut_implicit_schm[k*m+m-1, :].cuda()\n",
        "\n",
        "        phy_time = (k+1) * h_t\n",
        "\n",
        "        if k == 0:\n",
        "          net_u = NPDHG_solver_on_single_time_interval(device, save_path, net_u_laststep, phy_time, h_t, L, ave_value_ddW, N_r, N_b,\n",
        "                    minres_max_iter, minres_tol,\n",
        "                    network_length, hidden_dimension_net_u, hidden_dimension_net_phi, hidden_dimension_net_psi, flag_init,\n",
        "                    3000, phi_psi_iter, u_iter, omega, epsilon,\n",
        "                    plot_period, print_period, N_plot, chosen_dim_0, chosen_dim_1, flag_plot_real, benchmark_u, N_schm,\n",
        "                    number_stepsizes, base_stepsize,\n",
        "                    precond_type,\n",
        "                    bd_lambda,\n",
        "                    stepsize_0=0.2, adaptive_or_fixed_stepsize =\"fixed\", tau_u = tau_u, tau_phi = tau_phi, tau_psi = tau_psi )\n",
        "        else:\n",
        "          net_u = NPDHG_solver_on_single_time_interval(device, save_path, net_u_laststep, phy_time, h_t, L, ave_value_ddW, N_r, N_b,\n",
        "                      minres_max_iter, minres_tol,\n",
        "                      network_length, hidden_dimension_net_u, hidden_dimension_net_phi, hidden_dimension_net_psi, flag_init,\n",
        "                      iter, phi_psi_iter, u_iter, omega, epsilon,\n",
        "                      plot_period, print_period, N_plot, chosen_dim_0, chosen_dim_1, flag_plot_real, benchmark_u, N_schm,\n",
        "                      number_stepsizes, base_stepsize,\n",
        "                      precond_type,\n",
        "                      bd_lambda,\n",
        "                      stepsize_0=0.2, adaptive_or_fixed_stepsize =\"fixed\", tau_u = tau_u, tau_phi = tau_phi, tau_psi = tau_psi )\n",
        "        net_u_laststep = net_u\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "aGVmqhxnHkz8"
      },
      "outputs": [],
      "source": [
        "# @title  apply NPDHG_solver (with time causality) start from time=k_0*ht\n",
        "\n",
        "save_path = os.getcwd()\n",
        "\n",
        "# In Google Colab, change to working directory\n",
        "os.chdir(\"/content/drive/MyDrive/NPDG_RD\")\n",
        "# Verify\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "\n",
        "T =  0.04\n",
        "h_t = 0.001\n",
        "L=2\n",
        "\n",
        "ave_value_ddW = 2.0\n",
        "\n",
        "N_r = 2000\n",
        "N_b = 20\n",
        "\n",
        "iter_init_training = 20000\n",
        "\n",
        "minres_max_iter = 800\n",
        "minres_tol = 1e-4  # 1e-3\n",
        "\n",
        "network_length = 3\n",
        "hidden_dimension_net_u = 128\n",
        "hidden_dimension_net_phi = 128\n",
        "hidden_dimension_net_psi = 32\n",
        "\n",
        "flag_init = False\n",
        "\n",
        "iter = 6000\n",
        "phi_psi_iter = 1\n",
        "u_iter = 1\n",
        "omega = 1.0\n",
        "epsilon = 1\n",
        "\n",
        "plot_period  = 500\n",
        "print_period = 100\n",
        "N_plot = 100\n",
        "chosen_dim_0 = 0\n",
        "chosen_dim_1 = 0 # not used\n",
        "flag_plot_real = True\n",
        "m = 1\n",
        "N_space_discrt =   100\n",
        "Iter_num_fixed_pt = 8000\n",
        "\n",
        "number_stepsizes = 50\n",
        "base_stepsize = 0.8\n",
        "\n",
        "precond_type = \"RD\"\n",
        "\n",
        "bd_lambda = 1\n",
        "\n",
        "k_0 = 10\n",
        "\n",
        "net_u_k_0 = network_prim(network_length, dim, hidden_dimension_net_u, 1).to(device)\n",
        "net_u_k_0.load_state_dict(torch.load(os.path.join(save_path, 'netu_10.pt'), weights_only=True)) # suppose you computed to time = 10 * h_t\n",
        "\n",
        "NPDHG_solver_on_entire_time_interval_start_from_t0(k_0, net_u_k_0, device, save_path, T, h_t, L, ave_value_ddW, N_r, N_b,\n",
        "                          iter_init_training,\n",
        "                          minres_max_iter, minres_tol,\n",
        "                          network_length, hidden_dimension_net_u, hidden_dimension_net_phi, hidden_dimension_net_psi, flag_init,\n",
        "                          iter, phi_psi_iter, u_iter, omega, epsilon,\n",
        "                          plot_period, print_period, N_plot, chosen_dim_0, chosen_dim_1, flag_plot_real, m, N_space_discrt, Iter_num_fixed_pt, # m is the number of subintervals for IMEX scheme\n",
        "                          number_stepsizes, base_stepsize,\n",
        "                          precond_type,\n",
        "                          bd_lambda,\n",
        "                          stepsize_0=0.2,\n",
        "                          adaptive_or_fixed_stepsize =\"fixed\", tau_u = 0.01, tau_phi = 0.02, tau_psi = 0.02)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}